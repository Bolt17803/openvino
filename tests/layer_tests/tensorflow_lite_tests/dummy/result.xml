<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="5" failures="158" skipped="0" tests="168" time="63.817" timestamp="2024-01-25T16:56:04.322326" hostname="chaitanyasai-HP-ProBook-440-G5"><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7f29757ada20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="3.498" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7fd59e59cca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="3.349"><failure message="ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd582979d80&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7fd59e59cca0&gt;, 'op_name': 'POW', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryc3dh8ki7'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fd5828a2bf0&gt;, tensor_index = 0
value = array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.],
         [1...    [5.],
         [6.]],

        [[4.],
         [8.],
         [1.],
         [9.],
         [9.]]]], dtype=float32)

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f0607dd1e10&gt;, 'shape': [6, 7]} ]" time="3.243"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec3430a0&gt;
params = {'op_func': &lt;function equal at 0x7f0607dd1e10&gt;, 'op_name': 'EQUAL', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarysweufzjx'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05ec3911b0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f419471ef80&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.220"><failure message="struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f417867e380&gt;
params = {'op_func': &lt;function greater at 0x7f419471ef80&gt;, 'op_name': 'GREATER', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarytvn7qnyn'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
../../common/tflite_layer_test_class.py:54: in check_tflite_model_has_only_allowed_ops
    model = utils.read_model(self.model_path)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:61: in read_model
    model = convert_bytearray_to_object(model_bytearray)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:40: in convert_bytearray_to_object
    model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17784: in GetRootAsModel
    return cls.GetRootAs(buf, offset)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17776: in GetRootAs
    n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

packer_type = &lt;_struct.Struct object at 0x7f417b579cb0&gt;, buf = bytearray(b''), head = 0

    def Get(packer_type, buf, head):
        """ Get decodes a value at buf[head] using `packer_type`. """
&gt;       return packer_type.unpack_from(memoryview_type(buf), head)[0]
E       struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)

/home/chaitanyasai/.local/lib/python3.10/site-packages/flatbuffers/encode.py:26: error</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7f7d6c1170a0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.223"><failure message="AssertionError: TFLite model is not as you expect it to be: GREATER">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50272e60&gt;
params = {'op_func': &lt;function squared_difference at 0x7f7d6c1170a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryxjxhm7kg'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50272e60&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be: GREATER

../../common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f3acd79cca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="3.341"><failure message="ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b59ea0&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f3acd79cca0&gt;, 'op_name': 'POW', 'shape': [6, 7]}, ie_device = 'CPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarywiu1h7vo'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f3ab1a81990&gt;, tensor_index = 0
value = array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.],
         [1...    [5.],
         [6.]],

        [[4.],
         [8.],
         [1.],
         [9.],
         [9.]]]], dtype=float32)

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7fc8117a9870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="3.559"><failure message="ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75a0e0&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7fc8117a9870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarydjeth1c1'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc80f682e00&gt;, tensor_index = 0
value = array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.],
         [1...    [5.],
         [6.]],

        [[4.],
         [8.],
         [1.],
         [9.],
         [9.]]]], dtype=float32)

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7f7d6c2b5a20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="3.211"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502c4760&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7f7d6c2b5a20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarypf5g60lv'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d5019bc40&gt;&gt;
func_args = [{'Input_0': array([[[[False],
         [False],
         [False],
         [False],
         [False]],

        [[Fal...        [False]],

        [[False],
         [False],
         [False],
         [False],
         [False]]]])}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f29753ade10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.439"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cefb0&gt;
params = {'op_func': &lt;function equal at 0x7f29753ade10&gt;, 'op_name': 'EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryr3myh9qv'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f2959873ca0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7fc82b7c1ea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.755"><failure message="ValueError: Cannot set tensor: Got value of type BOOL but expected type FLOAT32 for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75a8c0&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7fc82b7c1ea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary21nk3j2_'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc8043aa6b0&gt;, tensor_index = 0
value = array([[[[False],
         [False],
         [False],
         [False],
         [False]],

        [[False],
        ...alse],
         [False]],

        [[False],
         [False],
         [False],
         [False],
         [False]]]])

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type BOOL but expected type FLOAT32 for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f06083a3250&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.464"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec343490&gt;
params = {'op_func': &lt;function greater_equal at 0x7f06083a3250&gt;, 'op_name': 'GREATER_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryxcv5hhhn'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05ec268a90&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f3acd79f6d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.731"><failure message="TypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b592d0&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7f3acd79f6d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryuzo0addf'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b592d0&gt;
infer_res = {'BinaryOperation': array([[[[ True],
         [False],
         [False],
         [False],
         [False]],

      ...lse],
         [False]],

        [[False],
         [False],
         [ True],
         [False],
         [False]]]])}
framework_res = {'BinaryOperation': array([[[[False],
         [False],
         [ True],
         [ True],
         [False]],

      ...rue],
         [False]],

        [[False],
         [ True],
         [False],
         [ True],
         [ True]]]])}
framework_eps = 0.0001

    def compare_ie_results_with_framework(self, infer_res, framework_res, framework_eps):
        is_ok = True
        from common.utils.common_utils import allclose
        for framework_out_name in framework_res:
            ie_out_name = framework_out_name
    
            if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
                            atol=framework_eps,
                            rtol=framework_eps):
                is_ok = False
                print("Max diff is {}".format(
                    np.array(
&gt;                       abs(infer_res[ie_out_name] - framework_res[framework_out_name])).max()))
E               TypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.

../../common/layer_test_class.py:166: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7fd59e5bde10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.607"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd58297af80&gt;
params = {'op_func': &lt;function equal at 0x7fd59e5bde10&gt;, 'op_name': 'EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryn6aiu4bb'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fd5781773d0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f419471ef80&gt;, 'shape': [3, 4, 5, 1]} ]" time="3.836"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f41786cc340&gt;
params = {'op_func': &lt;function greater at 0x7f419471ef80&gt;, 'op_name': 'GREATER', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryhe73m1e6'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41744595a0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'shape': [6, 7]} ]" time="0.729"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502c4a30&gt;
params = {'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary8j_q6tws'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d4c107460&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7f06081cdea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="1.004"><failure message="ValueError: Cannot set tensor: Got value of type BOOL but expected type FLOAT32 for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec341990&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7f06081cdea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryvkqhfjhd'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f05e4717880&gt;, tensor_index = 0
value = array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False],
  ...],
       [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False]])

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type BOOL but expected type FLOAT32 for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f3acd79cca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="1.089"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (POW) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5ae60&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f3acd79cca0&gt;, 'op_name': 'POW', 'shape': [6, 7]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarywz_npwht'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f3aa83273a0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (POW) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.684"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cf310&gt;
params = {'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'op_name': 'GREATER', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryla45c15i'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f29bcbb8ee0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7fc82b7c1ea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.814"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f7ac880&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7fc82b7c1ea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary_uz491tk'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fc80431b6a0&gt;&gt;
func_args = [{'Input_0': array([[[[False],
         [False],
         [False],
         [False],
         [False]],

        [[Fal...        [False]],

        [[False],
         [False],
         [False],
         [False],
         [False]]]])}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7fd5849a1990&gt;, 'shape': [6, 7]} ]" time="0.220"><failure message="AssertionError: TFLite model is not as you expect it to be: LOGICAL_OR">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd5829cca30&gt;
params = {'op_func': &lt;function maximum at 0x7fd5849a1990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary601d32rk'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd5829cca30&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be: LOGICAL_OR

../../common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7fd5849a1870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="0.562"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd58297b130&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7fd5849a1870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [6, 7]}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryh24zna7o'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fd578209bd0&gt;&gt;
func_args = [{'Input_0': array([[7., 1., 9., 7., 5., 8., 2.],
       [1., 4., 4., 3., 1., 8., 7.],
       [9., 5., 7., 5., 1., 3.,...., 4., 3., 7., 4., 2., 5.],
       [5., 6., 9., 9., 8., 6., 4.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f7d6be98ca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="0.577"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502c4d90&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f7d6be98ca0&gt;, 'op_name': 'POW', 'shape': [6, 7]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryqsuuzfme'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d50426860&gt;&gt;
func_args = [{'Input_0': array([[7., 1., 9., 7., 5., 8., 2.],
       [1., 4., 4., 3., 1., 8., 7.],
       [9., 5., 7., 5., 1., 3.,... 5., 7., 5., 6., 4.],
       [1., 4., 3., 7., 4., 2., 5.],
       [5., 6., 9., 9., 8., 6., 4.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f2975787250&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.946"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cd4b0&gt;
params = {'op_func': &lt;function greater_equal at 0x7f2975787250&gt;, 'op_name': 'GREATER_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryq6oq17l0'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f29bcba80d0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f06083a3250&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.671"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec390490&gt;
params = {'op_func': &lt;function greater_equal at 0x7f06083a3250&gt;, 'op_name': 'GREATER_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary3qax1ogy'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05ec1eba00&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7fc82b61f0a0&gt;, 'shape': [6, 7]} ]" time="0.966"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f759f30&gt;
params = {'op_func': &lt;function squared_difference at 0x7fc82b61f0a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [6, 7]}, ie_device = 'CPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryce9xnhu6'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc80431b280&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7fd59e59f6d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="1.143"><failure message="AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[8.235430e+05],&#10;         [1.000000e+00],&#10;         [7.290000e+02],&#10;         [3.430000e+02],&#10;         [7.812500e+04]],&#10;&#10;        [[3.276800e+04],&#10;         [1.280000e+02],&#10;         [1.000000e+00],&#10;         [4.096000e+03],&#10;         [2.560000e+02]],&#10;&#10;        [[3.000000e+00],&#10;         [1.000000e+00],&#10;         [5.120000e+02],&#10;         [8.235430e+05],&#10;         [6.561000e+03]],&#10;&#10;        [[2.500000e+01],&#10;         [1.680700e+04],&#10;         [3.125000e+03],&#10;         [1.000000e+00],&#10;         [1.968300e+04]]],&#10;&#10;&#10;       [[[2.621440e+05],&#10;         [1.679616e+06],&#10;         [6.400000e+01],&#10;         [2.560000e+02],&#10;         [1.024000e+03]],&#10;&#10;        [[2.621440e+05],&#10;         [1.000000e+00],&#10;         [1.679616e+06],&#10;         [1.600000e+01],&#10;         [3.200000e+01]],&#10;&#10;        [[4.665600e+04],&#10;         [4.900000e+01],&#10;         [6.553600e+04],&#10;         [3.125000e+03],&#10;         [1.280000e+02]],&#10;&#10;        [[5.764801e+06],&#10;         [5.120000e+02],&#10;         [1.600000e+01],&#10;         [1.600000e+01],&#10;         [1.000000e+00]]],&#10;&#10;&#10;       [[[2.560000e+02],&#10;         [2.560000e+02],&#10;         [1.000000e+00],&#10;         [1.562500e+04],&#10;         [4.000000e+00]],&#10;&#10;        [[4.665600e+04],&#10;         [1.000000e+00],&#10;         [1.176490e+05],&#10;         [8.100000e+01],&#10;         [1.968300e+04]],&#10;&#10;        [[1.600000e+01],&#10;         [7.000000e+00],&#10;         [1.600000e+01],&#10;         [5.000000e+00],&#10;         [2.799360e+05]],&#10;&#10;        [[2.621440e+05],&#10;         [2.097152e+06],&#10;         [1.000000e+00],&#10;         [8.100000e+01],&#10;         [4.782969e+06]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[False],&#10;         [ True],&#10;         [ True],&#10;         [ True],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [ True],&#10;         [ True],&#10;         [ True],&#10;         [False]],&#10;&#10;        [[ True],&#10;         [ True],&#10;         [ True],&#10;         [False],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [ True]]],&#10;&#10;&#10;       [[[ True],&#10;         [ True],&#10;         [ True],&#10;         [False],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [ True],&#10;         [ True],&#10;         [ True],&#10;         [ True]],&#10;&#10;        [[False],&#10;         [ True],&#10;         [ True],&#10;         [False],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [ True],&#10;         [ True],&#10;         [ True],&#10;         [ True]]],&#10;&#10;&#10;       [[[ True],&#10;         [False],&#10;         [ True],&#10;         [ True],&#10;         [ True]],&#10;&#10;        [[False],&#10;         [ True],&#10;         [ True],&#10;         [ True],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [ True],&#10;         [ True],&#10;         [ True],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [ True]]]])}.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd582979240&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7fd59e59f6d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary5zbd8olk'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd582979240&gt;
framework_model = node {
  name: "Input_0"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
       ...0"
  input: "Input_1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary5zbd8olk', use_old_api = False
use_new_frontend = True, infer_timeout = 60, enabled_transforms = '', disabled_transforms = ''
kwargs = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7fd59e59f6d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [3, 4, 5, 1]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite', compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...', 'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary5zbd8olk', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[8.235430e+05],
E                [1.000000e+00],
E                [7.290000e+02],
E                [3.430000e+02],
E                [7.812500e+04]],
E       
E               [[3.276800e+04],
E                [1.280000e+02],
E                [1.000000e+00],
E                [4.096000e+03],
E                [2.560000e+02]],
E       
E               [[3.000000e+00],
E                [1.000000e+00],
E                [5.120000e+02],
E                [8.235430e+05],
E                [6.561000e+03]],
E       
E               [[2.500000e+01],
E                [1.680700e+04],
E                [3.125000e+03],
E                [1.000000e+00],
E                [1.968300e+04]]],
E       
E       
E              [[[2.621440e+05],
E                [1.679616e+06],
E                [6.400000e+01],
E                [2.560000e+02],
E                [1.024000e+03]],
E       
E               [[2.621440e+05],
E                [1.000000e+00],
E                [1.679616e+06],
E                [1.600000e+01],
E                [3.200000e+01]],
E       
E               [[4.665600e+04],
E                [4.900000e+01],
E                [6.553600e+04],
E                [3.125000e+03],
E                [1.280000e+02]],
E       
E               [[5.764801e+06],
E                [5.120000e+02],
E                [1.600000e+01],
E                [1.600000e+01],
E                [1.000000e+00]]],
E       
E       
E              [[[2.560000e+02],
E                [2.560000e+02],
E                [1.000000e+00],
E                [1.562500e+04],
E                [4.000000e+00]],
E       
E               [[4.665600e+04],
E                [1.000000e+00],
E                [1.176490e+05],
E                [8.100000e+01],
E                [1.968300e+04]],
E       
E               [[1.600000e+01],
E                [7.000000e+00],
E                [1.600000e+01],
E                [5.000000e+00],
E                [2.799360e+05]],
E       
E               [[2.621440e+05],
E                [2.097152e+06],
E                [1.000000e+00],
E                [8.100000e+01],
E                [4.782969e+06]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[False],
E                [ True],
E                [ True],
E                [ True],
E                [ True]],
E       
E               [[ True],
E                [ True],
E                [ True],
E                [ True],
E                [False]],
E       
E               [[ True],
E                [ True],
E                [ True],
E                [False],
E                [ True]],
E       
E               [[ True],
E                [ True],
E                [False],
E                [ True],
E                [ True]]],
E       
E       
E              [[[ True],
E                [ True],
E                [ True],
E                [False],
E                [ True]],
E       
E               [[ True],
E                [ True],
E                [ True],
E                [ True],
E                [ True]],
E       
E               [[False],
E                [ True],
E                [ True],
E                [False],
E                [ True]],
E       
E               [[ True],
E                [ True],
E                [ True],
E                [ True],
E                [ True]]],
E       
E       
E              [[[ True],
E                [False],
E                [ True],
E                [ True],
E                [ True]],
E       
E               [[False],
E                [ True],
E                [ True],
E                [ True],
E                [ True]],
E       
E               [[ True],
E                [ True],
E                [ True],
E                [ True],
E                [ True]],
E       
E               [[ True],
E                [ True],
E                [False],
E                [ True],
E                [ True]]]])}.

../../common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f3acd79cca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.715"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5bd90&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f3acd79cca0&gt;, 'op_name': 'POW', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryg9_tm1gw'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3ab1a82020&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f7d6be9b6d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="0.875"><failure message="UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502712d0&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7f7d6be9b6d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [6, 7]}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarym82whaf1'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:87: in get_tflite_results
    output_details = interpreter.get_output_details()
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:759: in get_output_details
    return [
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:760: in &lt;listcomp&gt;
    self._get_tensor_details(i, subgraph_index=0)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7d5019b820&gt;, tensor_index = 2, subgraph_index = 0

    def _get_tensor_details(self, tensor_index, subgraph_index):
      """Gets tensor details.
    
      Args:
        tensor_index: Tensor index of tensor to query.
        subgraph_index: Index of the subgraph.
    
      Returns:
        A dictionary containing the following fields of the tensor:
          'name': The tensor name.
          'index': The tensor index in the interpreter.
          'shape': The shape of the tensor.
          'quantization': Deprecated, use 'quantization_parameters'. This field
              only works for per-tensor quantization, whereas
              'quantization_parameters' works in all cases.
          'quantization_parameters': The parameters used to quantize the tensor:
            'scales': List of scales (one if per-tensor quantization)
            'zero_points': List of zero_points (one if per-tensor quantization)
            'quantized_dimension': Specifies the dimension of per-axis
                quantization, in the case of multiple scales/zero_points.
    
      Raises:
        ValueError: If tensor_index is invalid.
      """
      tensor_index = int(tensor_index)
      subgraph_index = int(subgraph_index)
&gt;     tensor_name = self._interpreter.TensorName(tensor_index, subgraph_index)
E     UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:612: UnicodeDecodeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS', 'op_func': &lt;function less at 0x7f06081ccc10&gt;, 'shape': [6, 7]} ]" time="0.763"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (GREATER_EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec3416c0&gt;
params = {'op_func': &lt;function less at 0x7f06081ccc10&gt;, 'op_name': 'LESS', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary9kuxep6d'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f05e47e7100&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (GREATER_EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'NOT_EQUAL', 'op_func': &lt;function not_equal at 0x7f29753adfc0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.865"><failure message="TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cdc90&gt;
params = {'op_func': &lt;function not_equal at 0x7f29753adfc0&gt;, 'op_name': 'NOT_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryh3gqtdr9'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
../../common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([[[[ True],
         [ True],
         [ True],
         [ True],
         [ True]],

        [[ True],
        ...True],
         [ True]],

        [[ True],
         [ True],
         [ True],
         [ True],
         [ True]]]])
ref_array = array([[[[ 8.],
         [ 8.],
         [-7.],
         [-2.],
         [ 3.]],

        [[-5.],
         [ 7.],
    ....],
         [ 9.]],

        [[-5.],
         [ 3.],
         [ 2.],
         [-4.],
         [ 4.]]]], dtype=float32)
atol = 0.0001, rtol = 0.0001

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
&gt;           abs_diff = np.absolute(cur_array ^ ref_array)
E           TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

../../common/utils/common_utils.py:100: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'NOT_EQUAL', 'op_func': &lt;function not_equal at 0x7f3acd7c1fc0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.643"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5bc70&gt;
params = {'op_func': &lt;function not_equal at 0x7f3acd7c1fc0&gt;, 'op_name': 'NOT_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryzx59g4h_'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3aa83ea0b0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7fc8117a9990&gt;, 'shape': [6, 7]} ]" time="0.793"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (MAXIMUM) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75aa70&gt;
params = {'op_func': &lt;function maximum at 0x7fc8117a9990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarykdniw8sb'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc8043abfd0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (MAXIMUM) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7fd59e997250&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.672"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd5829cc400&gt;
params = {'op_func': &lt;function greater_equal at 0x7fd59e997250&gt;, 'op_name': 'GREATER_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary11zr7s6m'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fd57824af20&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.671"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (GREATER_EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50271a20&gt;
params = {'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'op_name': 'MAXIMUM', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryuyjowshx'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7d4c0de2c0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (GREATER_EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f0607dd1e10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.910"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec342050&gt;
params = {'op_func': &lt;function equal at 0x7f0607dd1e10&gt;, 'op_name': 'EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary_yt8sazd'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f064f591510&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f3ab3ba9990&gt;, 'shape': [6, 7]} ]" time="0.765"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5ab00&gt;
params = {'op_func': &lt;function maximum at 0x7f3ab3ba9990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryy_0_6sr4'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f3b14fb5270&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f2975787250&gt;, 'shape': [6, 7]} ]" time="0.929"><failure message="ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597ce500&gt;
params = {'op_func': &lt;function greater_equal at 0x7f2975787250&gt;, 'op_name': 'GREATER_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary003564wl'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f29bcba91b0&gt;, tensor_index = 0
value = array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
       [  0.,  -...,
       [ -7.,   9.,  -7.,   6.,   7.,   7.,  -1.],
       [ -7.,   3.,  -6.,   2.,  -9.,  -4.,  -3.]], dtype=float32)

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7fd59e9bda20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.896"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd5829797e0&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7fd59e9bda20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary1vzko005'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fd57820a1d0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7fc82b3a0ca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.585"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f7acd00&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7fc82b3a0ca0&gt;, 'op_name': 'POW', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary8oorn46e'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fc878f198a0&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...,
         [7.]],

        [[9.],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f41943236d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.205"><failure message="struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f417867e260&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7f41943236d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary678muhci'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
../../common/tflite_layer_test_class.py:54: in check_tflite_model_has_only_allowed_ops
    model = utils.read_model(self.model_path)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:61: in read_model
    model = convert_bytearray_to_object(model_bytearray)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:40: in convert_bytearray_to_object
    model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17784: in GetRootAsModel
    return cls.GetRootAs(buf, offset)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17776: in GetRootAs
    n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

packer_type = &lt;_struct.Struct object at 0x7f417b579cb0&gt;, buf = bytearray(b''), head = 0

    def Get(packer_type, buf, head):
        """ Get decodes a value at buf[head] using `packer_type`. """
&gt;       return packer_type.unpack_from(memoryview_type(buf), head)[0]
E       struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)

/home/chaitanyasai/.local/lib/python3.10/site-packages/flatbuffers/encode.py:26: error</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7f7d6c2b5a20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="0.761"><failure message="ValueError: Cannot set tensor: Got value of type BOOL but expected type FLOAT32 for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50272830&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7f7d6c2b5a20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary33dk7m1b'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7db3562020&gt;, tensor_index = 0
value = array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False],
  ...],
       [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False]])

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type BOOL but expected type FLOAT32 for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7fc82b7c1ea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="1.014"><failure message="ValueError: Cannot set tensor: Got value of type BOOL but expected type FLOAT32 for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f759900&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7fc82b7c1ea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryhr_ood_m'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc8043c2c80&gt;, tensor_index = 0
value = array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False],
  ...],
       [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False]])

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type BOOL but expected type FLOAT32 for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7f06081cdea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.700"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec390910&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7f06081cdea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryhpmp3hmo'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05e47b26b0&gt;&gt;
func_args = [{'Input_0': array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, Fals...  [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False]])}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f3acd79f6d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="1.001" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'shape': [6, 7]} ]" time="0.706"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cf3a0&gt;
params = {'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'op_name': 'GREATER', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarykuuvntuf'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f29bcbfc700&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7fd59e9bcee0&gt;, 'shape': [6, 7]} ]" time="0.900"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (NOT_EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd582979750&gt;
params = {'op_func': &lt;function less_equal at 0x7fd59e9bcee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarybamvuf4r'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fd5e5d5eb30&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (NOT_EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f7d6be9b6d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="0.472"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50273250&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7f7d6be9b6d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [6, 7]}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryt_15qfql'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7db350ec50&gt;&gt;
func_args = [{'Input_0': array([[7., 1., 9., 7., 5., 8., 2.],
       [1., 4., 4., 3., 1., 8., 7.],
       [9., 5., 7., 5., 1., 3.,...., 4., 3., 7., 4., 2., 5.],
       [5., 6., 9., 9., 8., 6., 4.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'NOT_EQUAL', 'op_func': &lt;function not_equal at 0x7f0607dd1fc0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.702"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec341cf0&gt;
params = {'op_func': &lt;function not_equal at 0x7f0607dd1fc0&gt;, 'op_name': 'NOT_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryzzlde8_i'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f05e4717730&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f3acd79f6d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.537"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5b250&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7f3acd79f6d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarybmzor0hp'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3aa83b78e0&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS', 'op_func': &lt;function less at 0x7f29757acc10&gt;, 'shape': [6, 7]} ]" time="0.222"><failure message="struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cd660&gt;
params = {'op_func': &lt;function less at 0x7f29757acc10&gt;, 'op_name': 'LESS', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary3duzr3fe'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
../../common/tflite_layer_test_class.py:54: in check_tflite_model_has_only_allowed_ops
    model = utils.read_model(self.model_path)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:61: in read_model
    model = convert_bytearray_to_object(model_bytearray)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:40: in convert_bytearray_to_object
    model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17784: in GetRootAsModel
    return cls.GetRootAs(buf, offset)
/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17776: in GetRootAs
    n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

packer_type = &lt;_struct.Struct object at 0x7f295c691a30&gt;, buf = bytearray(b''), head = 0

    def Get(packer_type, buf, head):
        """ Get decodes a value at buf[head] using `packer_type`. """
&gt;       return packer_type.unpack_from(memoryview_type(buf), head)[0]
E       struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)

/home/chaitanyasai/.local/lib/python3.10/site-packages/flatbuffers/encode.py:26: error</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7fc82b7c1ea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.511"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75b880&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7fc82b7c1ea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryjdcq34k7'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fc878f83160&gt;&gt;
func_args = [{'Input_0': array([[[[False],
         [False],
         [False],
         [False],
         [False]],

        [[Fal...lse],
         [False],
         [False],
         [False],
         [False]]]])}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7f7d6c1170a0&gt;, 'shape': [6, 7]} ]" time="0.660"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50273eb0&gt;
params = {'op_func': &lt;function squared_difference at 0x7f7d6c1170a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [6, 7]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryrvswrko3'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d4c105420&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7fd59e9bda20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="0.549"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd58297b7f0&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7fd59e9bda20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarydljy0ibi'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fd5782163e0&gt;&gt;
func_args = [{'Input_0': array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, Fals...False, False, False],
       [False, False, False, False, False, False, False]])}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'NOT_EQUAL', 'op_func': &lt;function not_equal at 0x7f0607dd1fc0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.608"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec343c70&gt;
params = {'op_func': &lt;function not_equal at 0x7f0607dd1fc0&gt;, 'op_name': 'NOT_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary00r7p4fc'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05e4717a30&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LESS', 'op_func': &lt;function less at 0x7f3acdbbcc10&gt;, 'shape': [6, 7]} ]" time="0.648"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5b640&gt;
params = {'op_func': &lt;function less at 0x7f3acdbbcc10&gt;, 'op_name': 'LESS', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryojx8o76e'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3b14fc8430&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7fc82b3a0ca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.674"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75bd00&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7fc82b3a0ca0&gt;, 'op_name': 'POW', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarykfr6kv85'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fc878ffda80&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7fd59e9bda20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="0.742"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd5829cc7f0&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7fd59e9bda20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryc8f5hjbr'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fd57820b790&gt;&gt;
func_args = [{'Input_0': array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, Fals...  [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False]])}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'shape': [6, 7]} ]" time="0.866"><failure message="ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50271ab0&gt;
params = {'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarys_arrdrs'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7d4c0d88b0&gt;, tensor_index = 0
value = array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
       [  0.,  -...,
       [ -7.,   9.,  -7.,   6.,   7.,   7.,  -1.],
       [ -7.,   3.,  -6.,   2.,  -9.,  -4.,  -3.]], dtype=float32)

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7f3acdbbdea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.689"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5b910&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7f3acdbbdea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary5b41pujo'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3b14f80f40&gt;&gt;
func_args = [{'Input_0': array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, Fals...False, False, False],
       [False, False, False, False, False, False, False]])}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'MINIMUM', 'op_func': &lt;function minimum at 0x7fc8117a9a20&gt;, 'shape': [3, 4, 5, 1]} ]" time="1.013"><failure message="AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[ 7.,  0.,  6., -1.,  2., -6.,  7.],&#10;       [ 2.,  5.,  1.,  9., -7., -5.,  0.],&#10;       [ 3.,  2.,  1.,  0.,  8.,  8., -4.],&#10;       [-1.,  6.,  3.,  8.,  9.,  3.,  7.],&#10;       [ 3.,  9.,  8.,  6.,  8.,  7., -1.],&#10;       [ 0.,  3., -6.,  5.,  6., -4.,  8.]], dtype=float32)}; framework_res={'BinaryOperation': array([[ True, False,  True,  True, False, False, False],&#10;       [ True, False,  True, False,  True,  True, False],&#10;       [ True,  True, False, False,  True,  True, False],&#10;       [False, False, False, False,  True,  True, False],&#10;       [ True, False,  True, False,  True, False, False],&#10;       [ True, False, False,  True,  True, False,  True]])}.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f759b40&gt;
params = {'op_func': &lt;function minimum at 0x7fc8117a9a20&gt;, 'op_name': 'MINIMUM', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryyfwxg8_z'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f759b40&gt;
framework_model = node {
  name: "Input_0"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
       ...0"
  input: "Input_1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryyfwxg8_z', use_old_api = False
use_new_frontend = True, infer_timeout = 60, enabled_transforms = '', disabled_transforms = ''
kwargs = {'op_func': &lt;function minimum at 0x7fc8117a9a20&gt;, 'op_name': 'MINIMUM', 'shape': [3, 4, 5, 1]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite', compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...', 'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryyfwxg8_z', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[ 7.,  0.,  6., -1.,  2., -6.,  7.],
E              [ 2.,  5.,  1.,  9., -7., -5.,  0.],
E              [ 3.,  2.,  1.,  0.,  8.,  8., -4.],
E              [-1.,  6.,  3.,  8.,  9.,  3.,  7.],
E              [ 3.,  9.,  8.,  6.,  8.,  7., -1.],
E              [ 0.,  3., -6.,  5.,  6., -4.,  8.]], dtype=float32)}; framework_res={'BinaryOperation': array([[ True, False,  True,  True, False, False, False],
E              [ True, False,  True, False,  True,  True, False],
E              [ True,  True, False, False,  True,  True, False],
E              [False, False, False, False,  True,  True, False],
E              [ True, False,  True, False,  True, False, False],
E              [ True, False, False,  True,  True, False,  True]])}.

../../common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f05ee365990&gt;, 'shape': [6, 7]} ]" time="0.731"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec343ac0&gt;
params = {'op_func': &lt;function maximum at 0x7f05ee365990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryqesbq5kq'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f064f5a63e0&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7fd59e9bcee0&gt;, 'shape': [3, 4, 5, 1]} ]" time="1.024"><failure message="TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd58297a680&gt;
params = {'op_func': &lt;function less_equal at 0x7fd59e9bcee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryj_1p0zui'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
../../common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([[[[ True],
         [ True],
         [ True],
         [False],
         [ True]],

        [[ True],
        ...True],
         [ True]],

        [[ True],
         [False],
         [False],
         [False],
         [ True]]]])
ref_array = array([[[[ 1.0000000e+00],
         [ 0.0000000e+00],
         [-1.0000000e-07],
         [-7.8125000e-03],
         [...   [ 3.0000000e+00],
         [ 2.0000000e+00],
         [ 9.5367432e-07],
         [ 1.0000000e+00]]]], dtype=float32)
atol = 0.05, rtol = 0.05

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
&gt;           abs_diff = np.absolute(cur_array ^ ref_array)
E           TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

../../common/utils/common_utils.py:100: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f7d6c292f80&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.659"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502c42e0&gt;
params = {'op_func': &lt;function greater at 0x7f7d6c292f80&gt;, 'op_name': 'GREATER', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryryls6bhx'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d4c0ddab0&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7f4194740ee0&gt;, 'shape': [6, 7]} ]" time="0.818"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f417867e770&gt;
params = {'op_func': &lt;function less_equal at 0x7f4194740ee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryhkitnz99'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f417445bbb0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f3ab3ba9990&gt;, 'shape': [6, 7]} ]" time="0.621"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5bac0&gt;
params = {'op_func': &lt;function maximum at 0x7f3ab3ba9990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarybkaztu2i'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3aa83b4550&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'MINIMUM', 'op_func': &lt;function minimum at 0x7fc8117a9a20&gt;, 'shape': [6, 7]} ]" time="0.523"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75bb50&gt;
params = {'op_func': &lt;function minimum at 0x7fc8117a9a20&gt;, 'op_name': 'MINIMUM', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarywtty5y3g'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fc878f818d0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.622"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec341e10&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'op_name': 'POW', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary7nkj9wct'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f064f5903d0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LESS', 'op_func': &lt;function less at 0x7f7d6c2b4c10&gt;, 'shape': [6, 7]} ]" time="0.717"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502735b0&gt;
params = {'op_func': &lt;function less at 0x7f7d6c2b4c10&gt;, 'op_name': 'LESS', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarykc2393ag'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7db35bb4f0&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7fd59e5bde10&gt;, 'shape': [6, 7]} ]" time="0.735"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd58297bfd0&gt;
params = {'op_func': &lt;function equal at 0x7fd59e5bde10&gt;, 'op_name': 'EQUAL', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryebscabz0'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fd57820f970&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'NOT_EQUAL', 'op_func': &lt;function not_equal at 0x7f4194345fc0&gt;, 'shape': [6, 7]} ]" time="1.278"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LOGICAL_OR) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f417867dd50&gt;
params = {'op_func': &lt;function not_equal at 0x7f4194345fc0&gt;, 'op_name': 'NOT_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary6eh_t355'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f41744b5d50&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LOGICAL_OR) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7f3acdbbdea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="1.176" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7fc82b79af80&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.989"><failure message="ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75a320&gt;
params = {'op_func': &lt;function greater at 0x7fc82b79af80&gt;, 'op_name': 'GREATER', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary35f4upg_'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc878f79420&gt;, tensor_index = 0
value = array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
         [  7....       [ -5.]],

        [[-10.],
         [  3.],
         [  2.],
         [ -4.],
         [ -1.]]]], dtype=float32)

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f06083a3250&gt;, 'shape': [6, 7]} ]" time="1.032"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (GREATER_EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec342560&gt;
params = {'op_func': &lt;function greater_equal at 0x7f06083a3250&gt;, 'op_name': 'GREATER_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarybdzx2ov4'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f05e47c1270&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (GREATER_EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f7d6beb9e10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.675"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50273f40&gt;
params = {'op_func': &lt;function equal at 0x7f7d6beb9e10&gt;, 'op_name': 'EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary5lawy3vw'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7db35cbb80&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7fd59e9bdea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.546"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd58297b880&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7fd59e9bdea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary069nbtve'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fd578217fa0&gt;&gt;
func_args = [{'Input_0': array([[[[False],
         [False],
         [False],
         [False],
         [False]],

        [[Fal...lse],
         [False],
         [False],
         [False],
         [False]]]])}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f3acdb97250&gt;, 'shape': [6, 7]} ]" time="1.088"><failure message="TypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5a560&gt;
params = {'op_func': &lt;function greater_equal at 0x7f3acdb97250&gt;, 'op_name': 'GREATER_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryk2zpuk64'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5a560&gt;
infer_res = {'BinaryOperation': array([[False,  True, False, False,  True,  True,  True],
       [False,  True, False,  True, Fals...,
       [False,  True, False,  True, False,  True,  True],
       [False,  True,  True, False, False,  True, False]])}
framework_res = {'BinaryOperation': array([[ True, False,  True,  True, False, False, False],
       [ True, False,  True, False,  Tru...,
       [ True, False,  True, False,  True, False, False],
       [ True, False, False,  True,  True, False,  True]])}
framework_eps = 0.05

    def compare_ie_results_with_framework(self, infer_res, framework_res, framework_eps):
        is_ok = True
        from common.utils.common_utils import allclose
        for framework_out_name in framework_res:
            ie_out_name = framework_out_name
    
            if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
                            atol=framework_eps,
                            rtol=framework_eps):
                is_ok = False
                print("Max diff is {}".format(
                    np.array(
&gt;                       abs(infer_res[ie_out_name] - framework_res[framework_out_name])).max()))
E               TypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.

../../common/layer_test_class.py:166: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f41943236d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.007"><error message="failed on setup with &quot;worker 'gw4' crashed while running &quot;tensorflow_lite_tests/dummy/test_tfl_Binary.py::TestTFLiteBinaryLayerTest::test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f41943236d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]&quot;&quot;">worker 'gw4' crashed while running "tensorflow_lite_tests/dummy/test_tfl_Binary.py::TestTFLiteBinaryLayerTest::test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f41943236d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]"</error></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7f7d6c1170a0&gt;, 'shape': [6, 7]} ]" time="1.232"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LESS_EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50272ef0&gt;
params = {'op_func': &lt;function squared_difference at 0x7f7d6c1170a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [6, 7]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryfzqq_jmp'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7d4c0dc790&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LESS_EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7fd59e81b0a0&gt;, 'shape': [6, 7]} ]" time="1.131"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LESS_EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd58297aef0&gt;
params = {'op_func': &lt;function squared_difference at 0x7fd59e81b0a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [6, 7]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryfdp2muf5'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fd5e5d12c20&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LESS_EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f29753ade10&gt;, 'shape': [6, 7]} ]" time="0.156"><failure message="AssertionError: TFLite model is not as you expect it to be: SQUARED_DIFFERENCE">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597ce080&gt;
params = {'op_func': &lt;function equal at 0x7f29753ade10&gt;, 'op_name': 'EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarymefs7pht'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597ce080&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be: SQUARED_DIFFERENCE

../../common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7fc82b7c0ee0&gt;, 'shape': [6, 7]} ]" time="1.040"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LESS_EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75a710&gt;
params = {'op_func': &lt;function less_equal at 0x7fc82b7c0ee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary347egn3d'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc878f80ee0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LESS_EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f06083a2f80&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.677"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec390370&gt;
params = {'op_func': &lt;function greater at 0x7f06083a2f80&gt;, 'op_name': 'GREATER', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryjzduzjra'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05e47ade10&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7f29757acee0&gt;, 'shape': [6, 7]} ]" time="1.152"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cd780&gt;
params = {'op_func': &lt;function less_equal at 0x7f29757acee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarysm5th63_'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f2959873d00&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.797"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec390d90&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'op_name': 'POW', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryyoxdvddu'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05e47b0880&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...,
         [7.]],

        [[9.],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f3acd79cca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="0.743"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5be20&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f3acd79cca0&gt;, 'op_name': 'POW', 'shape': [6, 7]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryontddqg1'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3aa83f00d0&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7f295b7f9870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="0.526"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f2959820160&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7f295b7f9870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [6, 7]}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarybz9d13v3'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f29bc7abeb0&gt;&gt;
func_args = [{'Input_0': array([[7., 1., 9., 7., 5., 8., 2.],
       [1., 4., 4., 3., 1., 8., 7.],
       [9., 5., 7., 5., 1., 3.,... 5., 7., 5., 6., 4.],
       [1., 4., 3., 7., 4., 2., 5.],
       [5., 6., 9., 9., 8., 6., 4.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7fc82b3c1e10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.847"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f70f5b0&gt;
params = {'op_func': &lt;function equal at 0x7fc82b3c1e10&gt;, 'op_name': 'EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryu388cp9z'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc80431be50&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7f06081ccee0&gt;, 'shape': [3, 4, 5, 1]} ]" time="1.644"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec3906d0&gt;
params = {'op_func': &lt;function less_equal at 0x7f06081ccee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary20ctob7e'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f064f5ef7f0&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7f3ab3ba9870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.584"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1ba8130&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7f3ab3ba9870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarye9y628vk'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3aa83b2620&gt;&gt;
func_args = [{'Input_0': array([[7., 1., 9., 7., 5., 8., 2.],
       [1., 4., 4., 3., 1., 8., 7.],
       [9., 5., 7., 5., 1., 3.,... 5., 7., 5., 6., 4.],
       [1., 4., 3., 7., 4., 2., 5.],
       [5., 6., 9., 9., 8., 6., 4.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7fd59e996f80&gt;, 'shape': [6, 7]} ]" time="0.673"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd5829cc370&gt;
params = {'op_func': &lt;function greater at 0x7fd59e996f80&gt;, 'op_name': 'GREATER', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryys6n8ni4'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fd57820b220&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f7d6beb9e10&gt;, 'shape': [6, 7]} ]" time="0.881"><failure message="ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50272050&gt;
params = {'op_func': &lt;function equal at 0x7f7d6beb9e10&gt;, 'op_name': 'EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary_p8z9jvo'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7d9f45c880&gt;, tensor_index = 0
value = array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
       [  0.,  -...,
       [ -7.,   9.,  -7.,   6.,   7.,   7.,  -1.],
       [ -7.,   3.,  -6.,   2.,  -9.,  -4.,  -3.]], dtype=float32)

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7f29757adea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="1.220"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LESS) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597ce980&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7f29757adea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarywo3jp2gd'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f29bcbab6a0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LESS) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f3acdb97250&gt;, 'shape': [6, 7]} ]" time="1.339"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5b520&gt;
params = {'op_func': &lt;function greater_equal at 0x7f3acdb97250&gt;, 'op_name': 'GREATER_EQUAL', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary6owsu15o'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3aa83b7370&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7fc82b7c1a20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="0.661"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f7ac7f0&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7fc82b7c1a20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryv1rkqn6s'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fc878fe59c0&gt;&gt;
func_args = [{'Input_0': array([[[[False],
         [False],
         [False],
         [False],
         [False]],

        [[Fal...        [False]],

        [[False],
         [False],
         [False],
         [False],
         [False]]]])}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7fd5849a1990&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.845"><failure message="AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[ 8.],&#10;         [ 8.],&#10;         [-7.],&#10;         [-2.],&#10;         [ 3.]],&#10;&#10;        [[-5.],&#10;         [ 7.],&#10;         [ 9.],&#10;         [ 5.],&#10;         [-3.]],&#10;&#10;        [[ 9.],&#10;         [-8.],&#10;         [ 8.],&#10;         [ 0.],&#10;         [ 8.]],&#10;&#10;        [[-1.],&#10;         [ 1.],&#10;         [ 0.],&#10;         [ 2.],&#10;         [-6.]]],&#10;&#10;&#10;       [[[ 5.],&#10;         [ 6.],&#10;         [ 6.],&#10;         [ 8.],&#10;         [ 8.]],&#10;&#10;        [[ 9.],&#10;         [ 7.],&#10;         [ 7.],&#10;         [ 0.],&#10;         [ 9.]],&#10;&#10;        [[-5.],&#10;         [ 6.],&#10;         [ 7.],&#10;         [ 7.],&#10;         [-1.]],&#10;&#10;        [[-7.],&#10;         [ 3.],&#10;         [ 9.],&#10;         [ 7.],&#10;         [-3.]]],&#10;&#10;&#10;       [[[ 5.],&#10;         [ 4.],&#10;         [ 7.],&#10;         [-1.],&#10;         [ 6.]],&#10;&#10;        [[-1.],&#10;         [ 9.],&#10;         [-7.],&#10;         [-5.],&#10;         [ 2.]],&#10;&#10;        [[ 4.],&#10;         [ 2.],&#10;         [-1.],&#10;         [ 3.],&#10;         [ 9.]],&#10;&#10;        [[-5.],&#10;         [ 3.],&#10;         [ 2.],&#10;         [-4.],&#10;         [ 4.]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[ True],&#10;         [ True],&#10;         [ True],&#10;         [False],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [False],&#10;         [ True],&#10;         [False],&#10;         [ True]],&#10;&#10;        [[False],&#10;         [False],&#10;         [ True],&#10;         [False],&#10;         [ True]],&#10;&#10;        [[False],&#10;         [False],&#10;         [False],&#10;         [False],&#10;         [False]]],&#10;&#10;&#10;       [[[ True],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [False]],&#10;&#10;        [[False],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [False]],&#10;&#10;        [[ True],&#10;         [False],&#10;         [False],&#10;         [False],&#10;         [False]],&#10;&#10;        [[False],&#10;         [False],&#10;         [ True],&#10;         [ True],&#10;         [ True]]],&#10;&#10;&#10;       [[[ True],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [False]],&#10;&#10;        [[False],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [False]],&#10;&#10;        [[ True],&#10;         [ True],&#10;         [ True],&#10;         [ True],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [False],&#10;         [False],&#10;         [False],&#10;         [ True]]]])}.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd582979a20&gt;
params = {'op_func': &lt;function maximum at 0x7fd5849a1990&gt;, 'op_name': 'MAXIMUM', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary0rllc11s'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fd582979a20&gt;
framework_model = node {
  name: "Input_0"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
       ...0"
  input: "Input_1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary0rllc11s', use_old_api = False
use_new_frontend = True, infer_timeout = 60, enabled_transforms = '', disabled_transforms = ''
kwargs = {'op_func': &lt;function maximum at 0x7fd5849a1990&gt;, 'op_name': 'MAXIMUM', 'shape': [3, 4, 5, 1]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite', compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...', 'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary0rllc11s', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[ 8.],
E                [ 8.],
E                [-7.],
E                [-2.],
E                [ 3.]],
E       
E               [[-5.],
E                [ 7.],
E                [ 9.],
E                [ 5.],
E                [-3.]],
E       
E               [[ 9.],
E                [-8.],
E                [ 8.],
E                [ 0.],
E                [ 8.]],
E       
E               [[-1.],
E                [ 1.],
E                [ 0.],
E                [ 2.],
E                [-6.]]],
E       
E       
E              [[[ 5.],
E                [ 6.],
E                [ 6.],
E                [ 8.],
E                [ 8.]],
E       
E               [[ 9.],
E                [ 7.],
E                [ 7.],
E                [ 0.],
E                [ 9.]],
E       
E               [[-5.],
E                [ 6.],
E                [ 7.],
E                [ 7.],
E                [-1.]],
E       
E               [[-7.],
E                [ 3.],
E                [ 9.],
E                [ 7.],
E                [-3.]]],
E       
E       
E              [[[ 5.],
E                [ 4.],
E                [ 7.],
E                [-1.],
E                [ 6.]],
E       
E               [[-1.],
E                [ 9.],
E                [-7.],
E                [-5.],
E                [ 2.]],
E       
E               [[ 4.],
E                [ 2.],
E                [-1.],
E                [ 3.],
E                [ 9.]],
E       
E               [[-5.],
E                [ 3.],
E                [ 2.],
E                [-4.],
E                [ 4.]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[ True],
E                [ True],
E                [ True],
E                [False],
E                [ True]],
E       
E               [[ True],
E                [False],
E                [ True],
E                [False],
E                [ True]],
E       
E               [[False],
E                [False],
E                [ True],
E                [False],
E                [ True]],
E       
E               [[False],
E                [False],
E                [False],
E                [False],
E                [False]]],
E       
E       
E              [[[ True],
E                [ True],
E                [False],
E                [ True],
E                [False]],
E       
E               [[False],
E                [ True],
E                [False],
E                [ True],
E                [False]],
E       
E               [[ True],
E                [False],
E                [False],
E                [False],
E                [False]],
E       
E               [[False],
E                [False],
E                [ True],
E                [ True],
E                [ True]]],
E       
E       
E              [[[ True],
E                [ True],
E                [False],
E                [ True],
E                [False]],
E       
E               [[False],
E                [ True],
E                [False],
E                [ True],
E                [False]],
E       
E               [[ True],
E                [ True],
E                [ True],
E                [ True],
E                [ True]],
E       
E               [[ True],
E                [False],
E                [False],
E                [False],
E                [ True]]]])}.

../../common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LESS', 'op_func': &lt;function less at 0x7f7d6c2b4c10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.716"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50273520&gt;
params = {'op_func': &lt;function less at 0x7f7d6c2b4c10&gt;, 'op_name': 'LESS', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryr1us67n9'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d50317d90&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7f05ee365870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="1.087"><failure message="AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[0., 1., 1., 1., 0., 1., 2.],&#10;       [1., 0., 4., 3., 1., 2., 3.],&#10;       [1., 0., 7., 5., 1., 0., 1.],&#10;       [0., 2., 4., 4., 4., 1., 2.],&#10;       [0., 2., 0., 0., 0., 1., 2.],&#10;       [2., 2., 2., 2., 1., 2., 0.]], dtype=float32)}; framework_res={'BinaryOperation': array([[False, False, False, False, False, False, False],&#10;       [False,  True, False, False, False, False, False],&#10;       [False, False, False, False, False,  True, False],&#10;       [False, False, False, False, False, False, False],&#10;       [False, False, False,  True,  True, False, False],&#10;       [False, False, False, False, False, False,  True]])}.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec342200&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7f05ee365870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [6, 7]}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarysnp0ai4g'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec342200&gt;
framework_model = node {
  name: "Input_0"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
       ...0"
  input: "Input_1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarysnp0ai4g', use_old_api = False
use_new_frontend = True, infer_timeout = 60, enabled_transforms = '', disabled_transforms = ''
kwargs = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7f05ee365870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [6, 7]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite', compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...', 'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarysnp0ai4g', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[0., 1., 1., 1., 0., 1., 2.],
E              [1., 0., 4., 3., 1., 2., 3.],
E              [1., 0., 7., 5., 1., 0., 1.],
E              [0., 2., 4., 4., 4., 1., 2.],
E              [0., 2., 0., 0., 0., 1., 2.],
E              [2., 2., 2., 2., 1., 2., 0.]], dtype=float32)}; framework_res={'BinaryOperation': array([[False, False, False, False, False, False, False],
E              [False,  True, False, False, False, False, False],
E              [False, False, False, False, False,  True, False],
E              [False, False, False, False, False, False, False],
E              [False, False, False,  True,  True, False, False],
E              [False, False, False, False, False, False,  True]])}.

../../common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7f297560b0a0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.606"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cfe50&gt;
params = {'op_func': &lt;function squared_difference at 0x7f297560b0a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryxdci84j6'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f29bc7aa740&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7fc82b79b250&gt;, 'shape': [6, 7]} ]" time="0.747"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f7ac490&gt;
params = {'op_func': &lt;function greater_equal at 0x7fc82b79b250&gt;, 'op_name': 'GREATER_EQUAL', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary4cojtc82'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fc878f194e0&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7f7d6c2b4ee0&gt;, 'shape': [6, 7]} ]" time="0.972"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (MINIMUM) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50271750&gt;
params = {'op_func': &lt;function less_equal at 0x7f7d6c2b4ee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryg77ytw6f'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7d4c04fb50&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (MINIMUM) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7fd59e5bde10&gt;, 'shape': [6, 7]} ]" time="0.023"><error message="failed on setup with &quot;worker 'gw3' crashed while running &quot;tensorflow_lite_tests/dummy/test_tfl_Binary.py::TestTFLiteBinaryLayerTest::test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7fd59e5bde10&gt;, 'shape': [6, 7]} ]&quot;&quot;">worker 'gw3' crashed while running "tensorflow_lite_tests/dummy/test_tfl_Binary.py::TestTFLiteBinaryLayerTest::test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7fd59e5bde10&gt;, 'shape': [6, 7]} ]"</error></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f3acd7c1e10&gt;, 'shape': [6, 7]} ]" time="0.711"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1ba80a0&gt;
params = {'op_func': &lt;function equal at 0x7f3acd7c1e10&gt;, 'op_name': 'EQUAL', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarykcktxcdp'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3b00e5f9a0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'MINIMUM', 'op_func': &lt;function minimum at 0x7f295b7f9a20&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.828"><failure message="AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[ -1.],&#10;         [  0.],&#10;         [-10.],&#10;         [ -7.],&#10;         [  2.]],&#10;&#10;        [[ -6.],&#10;         [  2.],&#10;         [-10.],&#10;         [  3.],&#10;         [ -7.]],&#10;&#10;        [[  3.],&#10;         [-10.],&#10;         [-10.],&#10;         [ -8.],&#10;         [  0.]],&#10;&#10;        [[ -4.],&#10;         [ -4.],&#10;         [  0.],&#10;         [ -5.],&#10;         [ -7.]]],&#10;&#10;&#10;       [[[ -4.],&#10;         [ -1.],&#10;         [ -7.],&#10;         [  3.],&#10;         [ -7.]],&#10;&#10;        [[  1.],&#10;         [ -5.],&#10;         [  0.],&#10;         [ -7.],&#10;         [  2.]],&#10;&#10;        [[ -7.],&#10;         [  3.],&#10;         [  3.],&#10;         [  4.],&#10;         [ -3.]],&#10;&#10;        [[ -7.],&#10;         [ -2.],&#10;         [ -6.],&#10;         [  2.],&#10;         [ -9.]]],&#10;&#10;&#10;       [[[ -4.],&#10;         [ -3.],&#10;         [ -4.],&#10;         [ -9.],&#10;         [  0.]],&#10;&#10;        [[ -8.],&#10;         [ -9.],&#10;         [ -7.],&#10;         [-10.],&#10;         [ -3.]],&#10;&#10;        [[  0.],&#10;         [  1.],&#10;         [ -6.],&#10;         [ -7.],&#10;         [ -5.]],&#10;&#10;        [[-10.],&#10;         [  1.],&#10;         [  1.],&#10;         [-10.],&#10;         [ -1.]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[ True],&#10;         [ True],&#10;         [ True],&#10;         [False],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [False],&#10;         [ True],&#10;         [False],&#10;         [ True]],&#10;&#10;        [[False],&#10;         [False],&#10;         [ True],&#10;         [False],&#10;         [ True]],&#10;&#10;        [[False],&#10;         [False],&#10;         [False],&#10;         [False],&#10;         [False]]],&#10;&#10;&#10;       [[[ True],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [False]],&#10;&#10;        [[False],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [False]],&#10;&#10;        [[ True],&#10;         [False],&#10;         [False],&#10;         [False],&#10;         [False]],&#10;&#10;        [[False],&#10;         [False],&#10;         [ True],&#10;         [ True],&#10;         [ True]]],&#10;&#10;&#10;       [[[ True],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [False]],&#10;&#10;        [[False],&#10;         [ True],&#10;         [False],&#10;         [ True],&#10;         [False]],&#10;&#10;        [[ True],&#10;         [ True],&#10;         [ True],&#10;         [ True],&#10;         [ True]],&#10;&#10;        [[ True],&#10;         [False],&#10;         [False],&#10;         [False],&#10;         [ True]]]])}.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cdb70&gt;
params = {'op_func': &lt;function minimum at 0x7f295b7f9a20&gt;, 'op_name': 'MINIMUM', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary8f4_cfyl'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cdb70&gt;
framework_model = node {
  name: "Input_0"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
       ...0"
  input: "Input_1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary8f4_cfyl', use_old_api = False
use_new_frontend = True, infer_timeout = 60, enabled_transforms = '', disabled_transforms = ''
kwargs = {'op_func': &lt;function minimum at 0x7f295b7f9a20&gt;, 'op_name': 'MINIMUM', 'shape': [3, 4, 5, 1]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite', compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...', 'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary8f4_cfyl', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[ -1.],
E                [  0.],
E                [-10.],
E                [ -7.],
E                [  2.]],
E       
E               [[ -6.],
E                [  2.],
E                [-10.],
E                [  3.],
E                [ -7.]],
E       
E               [[  3.],
E                [-10.],
E                [-10.],
E                [ -8.],
E                [  0.]],
E       
E               [[ -4.],
E                [ -4.],
E                [  0.],
E                [ -5.],
E                [ -7.]]],
E       
E       
E              [[[ -4.],
E                [ -1.],
E                [ -7.],
E                [  3.],
E                [ -7.]],
E       
E               [[  1.],
E                [ -5.],
E                [  0.],
E                [ -7.],
E                [  2.]],
E       
E               [[ -7.],
E                [  3.],
E                [  3.],
E                [  4.],
E                [ -3.]],
E       
E               [[ -7.],
E                [ -2.],
E                [ -6.],
E                [  2.],
E                [ -9.]]],
E       
E       
E              [[[ -4.],
E                [ -3.],
E                [ -4.],
E                [ -9.],
E                [  0.]],
E       
E               [[ -8.],
E                [ -9.],
E                [ -7.],
E                [-10.],
E                [ -3.]],
E       
E               [[  0.],
E                [  1.],
E                [ -6.],
E                [ -7.],
E                [ -5.]],
E       
E               [[-10.],
E                [  1.],
E                [  1.],
E                [-10.],
E                [ -1.]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[ True],
E                [ True],
E                [ True],
E                [False],
E                [ True]],
E       
E               [[ True],
E                [False],
E                [ True],
E                [False],
E                [ True]],
E       
E               [[False],
E                [False],
E                [ True],
E                [False],
E                [ True]],
E       
E               [[False],
E                [False],
E                [False],
E                [False],
E                [False]]],
E       
E       
E              [[[ True],
E                [ True],
E                [False],
E                [ True],
E                [False]],
E       
E               [[False],
E                [ True],
E                [False],
E                [ True],
E                [False]],
E       
E               [[ True],
E                [False],
E                [False],
E                [False],
E                [False]],
E       
E               [[False],
E                [False],
E                [ True],
E                [ True],
E                [ True]]],
E       
E       
E              [[[ True],
E                [ True],
E                [False],
E                [ True],
E                [False]],
E       
E               [[False],
E                [ True],
E                [False],
E                [ True],
E                [False]],
E       
E               [[ True],
E                [ True],
E                [ True],
E                [ True],
E                [ True]],
E       
E               [[ True],
E                [False],
E                [False],
E                [False],
E                [ True]]]])}.

../../common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'NOT_EQUAL', 'op_func': &lt;function not_equal at 0x7fc82b3c1fc0&gt;, 'shape': [6, 7]} ]" time="0.628"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f7acc70&gt;
params = {'op_func': &lt;function not_equal at 0x7fc82b3c1fc0&gt;, 'op_name': 'NOT_EQUAL', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryrk28y3rv'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fc8043e9660&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS', 'op_func': &lt;function less at 0x7f06081ccc10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.850"><failure message="TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec341630&gt;
params = {'op_func': &lt;function less at 0x7f06081ccc10&gt;, 'op_name': 'LESS', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryps7lwjkp'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
../../common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([[[[ True],
         [ True],
         [ True],
         [False],
         [ True]],

        [[ True],
        ...True],
         [ True]],

        [[ True],
         [False],
         [False],
         [False],
         [ True]]]])
ref_array = array([[[[ 7.],
         [ 0.],
         [-3.],
         [-2.],
         [ 2.]],

        [[-1.],
         [ 1.],
    ....],
         [ 4.]],

        [[-0.],
         [ 0.],
         [ 0.],
         [-4.],
         [ 3.]]]], dtype=float32)
atol = 0.0001, rtol = 0.0001

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
&gt;           abs_diff = np.absolute(cur_array ^ ref_array)
E           TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

../../common/utils/common_utils.py:100: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS', 'op_func': &lt;function less at 0x7f7d6c2b4c10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.862"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_OR) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502715a0&gt;
params = {'op_func': &lt;function less at 0x7f7d6c2b4c10&gt;, 'op_name': 'LESS', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryk52mqimr'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7db35bcfd0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_OR) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7f3ab3ba9870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.750"><failure message="ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5a170&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7f3ab3ba9870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryb92v5bbf'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f3b14fdbca0&gt;, tensor_index = 0
value = array([[7., 1., 9., 7., 5., 8., 2.],
       [1., 4., 4., 3., 1., 8., 7.],
       [9., 5., 7., 5., 1., 3., 4.],
       [6., 2., 4., 4., 4., 1., 6.],
       [2., 2., 6., 7., 4., 5., 2.],
       [7., 8., 2., 2., 1., 2., 4.]], dtype=float32)

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7fc82b3a0ca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="0.877"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (FLOOR_MOD) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f759e10&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7fc82b3a0ca0&gt;, 'op_name': 'POW', 'shape': [6, 7]}, ie_device = 'CPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary5ubr4lmq'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc878facfa0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (FLOOR_MOD) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7f29757adea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="0.598"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cf940&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7f29757adea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary8e6jzan9'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f29bca797b0&gt;&gt;
func_args = [{'Input_0': array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, Fals...False, False, False],
       [False, False, False, False, False, False, False]])}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7f05ee365870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.691"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec343130&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7f05ee365870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary2_0vckne'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05e47afdc0&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7f7d6c2b4ee0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.662"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50273640&gt;
params = {'op_func': &lt;function less_equal at 0x7f7d6c2b4ee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarysn6yf_bo'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7db35c9630&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'shape': [6, 7]} ]" time="0.990"><failure message="TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cd420&gt;
params = {'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'op_name': 'GREATER', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary31mcsj3y'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
../../common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False],
  ...],
       [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False]])
ref_array = array([[ 7.,  0.,  6., -1.,  2., -6.,  7.],
       [ 2.,  5.,  1.,  9., -7., -5.,  0.],
       [ 3.,  2.,  1.,  0.,  8... 9.,  3.,  7.],
       [ 3.,  9.,  8.,  6.,  8.,  7., -1.],
       [ 0.,  3., -6.,  5.,  6., -4.,  8.]], dtype=float32)
atol = 0.0001, rtol = 0.0001

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
&gt;           abs_diff = np.absolute(cur_array ^ ref_array)
E           TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

../../common/utils/common_utils.py:100: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7f3acdbbcee0&gt;, 'shape': [6, 7]} ]" time="0.913"><failure message="TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b597e0&gt;
params = {'op_func': &lt;function less_equal at 0x7f3acdbbcee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary8isidbi0'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
../../common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False],
  ...],
       [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False]])
ref_array = array([[ 7.,  0.,  6., -1.,  2., -6.,  7.],
       [ 2.,  5.,  1.,  9., -7., -5.,  0.],
       [ 3.,  2.,  1.,  0.,  8... 9.,  3.,  7.],
       [ 3.,  9.,  8.,  6.,  8.,  7., -1.],
       [ 0.,  3., -6.,  5.,  6., -4.,  8.]], dtype=float32)
atol = 0.0001, rtol = 0.0001

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
&gt;           abs_diff = np.absolute(cur_array ^ ref_array)
E           TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

../../common/utils/common_utils.py:100: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7fc82b3c1e10&gt;, 'shape': [6, 7]} ]" time="0.884"><failure message="TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75a050&gt;
params = {'op_func': &lt;function equal at 0x7fc82b3c1e10&gt;, 'op_name': 'EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarynddika6g'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
../../common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False],
  ...],
       [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False]])
ref_array = array([[ 7.,  0.,  6., -1.,  2., -6.,  7.],
       [ 2.,  5.,  1.,  9., -7., -5.,  0.],
       [ 3.,  2.,  1.,  0.,  8... 9.,  3.,  7.],
       [ 3.,  9.,  8.,  6.,  8.,  7., -1.],
       [ 0.,  3., -6.,  5.,  6., -4.,  8.]], dtype=float32)
atol = 0.05, rtol = 0.05

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
&gt;           abs_diff = np.absolute(cur_array ^ ref_array)
E           TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

../../common/utils/common_utils.py:100: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7f05ee365870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.774"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec390130&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7f05ee365870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryd_h_olbi'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05e47e7c40&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...,
         [7.]],

        [[9.],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'shape': [6, 7]} ]" time="0.603"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50273a30&gt;
params = {'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarypp92g_4s'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d5019b580&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.686"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f2959820310&gt;
params = {'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'op_name': 'GREATER', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarymr2xqph8'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f29bcb2dc30&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f3acd7c1e10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.549"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5bfd0&gt;
params = {'op_func': &lt;function equal at 0x7f3acd7c1e10&gt;, 'op_name': 'EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary1rlxsns9'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3ab1a036a0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7fc82b79af80&gt;, 'shape': [6, 7]} ]" time="0.929"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f7593f0&gt;
params = {'op_func': &lt;function greater at 0x7fc82b79af80&gt;, 'op_name': 'GREATER', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryp048c7og'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc80f6837c0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7f7d6c2b4ee0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.965"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502716c0&gt;
params = {'op_func': &lt;function less_equal at 0x7f7d6c2b4ee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryui_8bpo0'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7db352f0d0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7f06081cda20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="0.597"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec390880&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7f06081cda20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary940dv2cd'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f064f501f60&gt;&gt;
func_args = [{'Input_0': array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, Fals...  [False, False, False, False, False, False, False],
       [False, False, False, False, False, False, False]])}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7f3acda1b0a0&gt;, 'shape': [6, 7]} ]" time="0.582"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1ba8f40&gt;
params = {'op_func': &lt;function squared_difference at 0x7f3acda1b0a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [6, 7]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryk7ttb7br'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3aa83b77c0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f297538cca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.685"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f2959820d30&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f297538cca0&gt;, 'op_name': 'POW', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryeqrh5ij_'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f29bca6ba00&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...,
         [7.]],

        [[9.],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f05ee365990&gt;, 'shape': [6, 7]} ]" time="0.760"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec390ac0&gt;
params = {'op_func': &lt;function maximum at 0x7f05ee365990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryyfs_6t5o'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05ec2f5e70&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LESS', 'op_func': &lt;function less at 0x7fc82b7c0c10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.884"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f75a560&gt;
params = {'op_func': &lt;function less at 0x7fc82b7c0c10&gt;, 'op_name': 'LESS', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryw86rr3f0'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7fc8043aefe0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f7d6be9b6d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.775"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50272200&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7f7d6be9b6d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary8yxyci6k'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7db35e8a00&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_AND) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7f3acdbbda20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="0.738"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5a8c0&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7f3acdbbda20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary0dxzfwwx'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f3aa83b0e80&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (POW) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'shape': [6, 7]} ]" time="0.825"><failure message="AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[ 1.0000000e+00],&#10;         [ 0.0000000e+00],&#10;         [-1.0000000e-07],&#10;         [-7.8125000e-03],&#10;         [ 8.0000000e+00]],&#10;&#10;        [[-1.2860082e-04],&#10;         [ 4.9000000e+01],&#10;         [-1.0000000e+09],&#10;         [ 1.2500000e+02],&#10;         [-2.9154520e-03]],&#10;&#10;        [[ 7.2900000e+02],&#10;         [ 9.3132257e-10],&#10;         [ 1.0000000e+08],&#10;         [           inf],&#10;         [ 0.0000000e+00]],&#10;&#10;        [[ 1.0000000e+00],&#10;         [ 1.0000000e+00],&#10;         [ 1.0000000e+00],&#10;         [ 3.1250000e-02],&#10;         [-3.5722451e-06]]],&#10;&#10;&#10;       [[[-1.0240000e+03],&#10;         [ 1.0000000e+00],&#10;         [ 3.5722451e-06],&#10;         [ 6.5610000e+03],&#10;         [ 4.7683716e-07]],&#10;&#10;        [[ 9.0000000e+00],&#10;         [-7.8125000e+04],&#10;         [ 1.0000000e+00],&#10;         [ 1.0000000e+00],&#10;         [ 8.1000000e+01]],&#10;&#10;        [[-5.9499020e-05],&#10;         [ 2.1600000e+02],&#10;         [ 3.4300000e+02],&#10;         [ 2.4010000e+03],&#10;         [-1.0000000e+00]],&#10;&#10;        [[-1.2142657e-06],&#10;         [ 1.1111111e-01],&#10;         [-1.0077696e+07],&#10;         [ 1.2800000e+02],&#10;         [-1.3717421e-03]]],&#10;&#10;&#10;       [[[-1.0240000e+03],&#10;         [ 8.1000000e+01],&#10;         [ 4.1649313e-04],&#10;         [-1.1111111e-01],&#10;         [ 1.0000000e+00]],&#10;&#10;        [[ 1.0000000e+00],&#10;         [-3.8742048e+08],&#10;         [-1.2142657e-06],&#10;         [-9.9999997e-06],&#10;         [ 1.2500000e-01]],&#10;&#10;        [[ 0.0000000e+00],&#10;         [ 1.0000000e+00],&#10;         [-1.6666667e-01],&#10;         [-3.4300000e+02],&#10;         [-1.9531250e+06]],&#10;&#10;        [[-9.9999997e-06],&#10;         [ 3.0000000e+00],&#10;         [ 2.0000000e+00],&#10;         [ 9.5367432e-07],&#10;         [ 1.0000000e+00]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[ 8.],&#10;         [ 8.],&#10;         [-7.],&#10;         [-2.],&#10;         [ 3.]],&#10;&#10;        [[-5.],&#10;         [ 7.],&#10;         [ 9.],&#10;         [ 5.],&#10;         [-3.]],&#10;&#10;        [[ 9.],&#10;         [-8.],&#10;         [ 8.],&#10;         [ 0.],&#10;         [ 8.]],&#10;&#10;        [[-1.],&#10;         [ 1.],&#10;         [ 0.],&#10;         [ 2.],&#10;         [-6.]]],&#10;&#10;&#10;       [[[ 5.],&#10;         [ 6.],&#10;         [ 6.],&#10;         [ 8.],&#10;         [ 8.]],&#10;&#10;        [[ 9.],&#10;         [ 7.],&#10;         [ 7.],&#10;         [ 0.],&#10;         [ 9.]],&#10;&#10;        [[-5.],&#10;         [ 6.],&#10;         [ 7.],&#10;         [ 7.],&#10;         [-1.]],&#10;&#10;        [[-7.],&#10;         [ 3.],&#10;         [ 9.],&#10;         [ 7.],&#10;         [-3.]]],&#10;&#10;&#10;       [[[ 5.],&#10;         [ 4.],&#10;         [ 7.],&#10;         [-1.],&#10;         [ 6.]],&#10;&#10;        [[-1.],&#10;         [ 9.],&#10;         [-7.],&#10;         [-5.],&#10;         [ 2.]],&#10;&#10;        [[ 4.],&#10;         [ 2.],&#10;         [-1.],&#10;         [ 3.],&#10;         [ 9.]],&#10;&#10;        [[-5.],&#10;         [ 3.],&#10;         [ 2.],&#10;         [-4.],&#10;         [ 4.]]]], dtype=float32)}.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597ce3e0&gt;
params = {'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'op_name': 'GREATER', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarykmiwjxu3'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597ce3e0&gt;
framework_model = node {
  name: "Input_0"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
       ...0"
  input: "Input_1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarykmiwjxu3', use_old_api = False
use_new_frontend = True, infer_timeout = 60, enabled_transforms = '', disabled_transforms = ''
kwargs = {'op_func': &lt;function greater at 0x7f2975786f80&gt;, 'op_name': 'GREATER', 'shape': [6, 7]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite', compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...', 'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarykmiwjxu3', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[ 1.0000000e+00],
E                [ 0.0000000e+00],
E                [-1.0000000e-07],
E                [-7.8125000e-03],
E                [ 8.0000000e+00]],
E       
E               [[-1.2860082e-04],
E                [ 4.9000000e+01],
E                [-1.0000000e+09],
E                [ 1.2500000e+02],
E                [-2.9154520e-03]],
E       
E               [[ 7.2900000e+02],
E                [ 9.3132257e-10],
E                [ 1.0000000e+08],
E                [           inf],
E                [ 0.0000000e+00]],
E       
E               [[ 1.0000000e+00],
E                [ 1.0000000e+00],
E                [ 1.0000000e+00],
E                [ 3.1250000e-02],
E                [-3.5722451e-06]]],
E       
E       
E              [[[-1.0240000e+03],
E                [ 1.0000000e+00],
E                [ 3.5722451e-06],
E                [ 6.5610000e+03],
E                [ 4.7683716e-07]],
E       
E               [[ 9.0000000e+00],
E                [-7.8125000e+04],
E                [ 1.0000000e+00],
E                [ 1.0000000e+00],
E                [ 8.1000000e+01]],
E       
E               [[-5.9499020e-05],
E                [ 2.1600000e+02],
E                [ 3.4300000e+02],
E                [ 2.4010000e+03],
E                [-1.0000000e+00]],
E       
E               [[-1.2142657e-06],
E                [ 1.1111111e-01],
E                [-1.0077696e+07],
E                [ 1.2800000e+02],
E                [-1.3717421e-03]]],
E       
E       
E              [[[-1.0240000e+03],
E                [ 8.1000000e+01],
E                [ 4.1649313e-04],
E                [-1.1111111e-01],
E                [ 1.0000000e+00]],
E       
E               [[ 1.0000000e+00],
E                [-3.8742048e+08],
E                [-1.2142657e-06],
E                [-9.9999997e-06],
E                [ 1.2500000e-01]],
E       
E               [[ 0.0000000e+00],
E                [ 1.0000000e+00],
E                [-1.6666667e-01],
E                [-3.4300000e+02],
E                [-1.9531250e+06]],
E       
E               [[-9.9999997e-06],
E                [ 3.0000000e+00],
E                [ 2.0000000e+00],
E                [ 9.5367432e-07],
E                [ 1.0000000e+00]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[ 8.],
E                [ 8.],
E                [-7.],
E                [-2.],
E                [ 3.]],
E       
E               [[-5.],
E                [ 7.],
E                [ 9.],
E                [ 5.],
E                [-3.]],
E       
E               [[ 9.],
E                [-8.],
E                [ 8.],
E                [ 0.],
E                [ 8.]],
E       
E               [[-1.],
E                [ 1.],
E                [ 0.],
E                [ 2.],
E                [-6.]]],
E       
E       
E              [[[ 5.],
E                [ 6.],
E                [ 6.],
E                [ 8.],
E                [ 8.]],
E       
E               [[ 9.],
E                [ 7.],
E                [ 7.],
E                [ 0.],
E                [ 9.]],
E       
E               [[-5.],
E                [ 6.],
E                [ 7.],
E                [ 7.],
E                [-1.]],
E       
E               [[-7.],
E                [ 3.],
E                [ 9.],
E                [ 7.],
E                [-3.]]],
E       
E       
E              [[[ 5.],
E                [ 4.],
E                [ 7.],
E                [-1.],
E                [ 6.]],
E       
E               [[-1.],
E                [ 9.],
E                [-7.],
E                [-5.],
E                [ 2.]],
E       
E               [[ 4.],
E                [ 2.],
E                [-1.],
E                [ 3.],
E                [ 9.]],
E       
E               [[-5.],
E                [ 3.],
E                [ 2.],
E                [-4.],
E                [ 4.]]]], dtype=float32)}.

../../common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.843"><failure message="AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[8.235430e+05],&#10;         [1.000000e+00],&#10;         [7.290000e+02],&#10;         [3.430000e+02],&#10;         [7.812500e+04]],&#10;&#10;        [[3.276800e+04],&#10;         [1.280000e+02],&#10;         [1.000000e+00],&#10;         [4.096000e+03],&#10;         [2.560000e+02]],&#10;&#10;        [[3.000000e+00],&#10;         [1.000000e+00],&#10;         [5.120000e+02],&#10;         [8.235430e+05],&#10;         [6.561000e+03]],&#10;&#10;        [[2.500000e+01],&#10;         [1.680700e+04],&#10;         [3.125000e+03],&#10;         [1.000000e+00],&#10;         [1.968300e+04]]],&#10;&#10;&#10;       [[[2.621440e+05],&#10;         [1.679616e+06],&#10;         [6.400000e+01],&#10;         [2.560000e+02],&#10;         [1.024000e+03]],&#10;&#10;        [[2.621440e+05],&#10;         [1.000000e+00],&#10;         [1.679616e+06],&#10;         [1.600000e+01],&#10;         [3.200000e+01]],&#10;&#10;        [[4.665600e+04],&#10;         [4.900000e+01],&#10;         [6.553600e+04],&#10;         [3.125000e+03],&#10;         [1.280000e+02]],&#10;&#10;        [[5.764801e+06],&#10;         [5.120000e+02],&#10;         [1.600000e+01],&#10;         [1.600000e+01],&#10;         [1.000000e+00]]],&#10;&#10;&#10;       [[[2.560000e+02],&#10;         [2.560000e+02],&#10;         [1.000000e+00],&#10;         [1.562500e+04],&#10;         [4.000000e+00]],&#10;&#10;        [[4.665600e+04],&#10;         [1.000000e+00],&#10;         [1.176490e+05],&#10;         [8.100000e+01],&#10;         [1.968300e+04]],&#10;&#10;        [[1.600000e+01],&#10;         [7.000000e+00],&#10;         [1.600000e+01],&#10;         [5.000000e+00],&#10;         [2.799360e+05]],&#10;&#10;        [[2.621440e+05],&#10;         [2.097152e+06],&#10;         [1.000000e+00],&#10;         [8.100000e+01],&#10;         [4.782969e+06]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[7.],&#10;         [3.],&#10;         [9.],&#10;         [7.],&#10;         [7.]],&#10;&#10;        [[8.],&#10;         [7.],&#10;         [5.],&#10;         [6.],&#10;         [4.]],&#10;&#10;        [[3.],&#10;         [4.],&#10;         [8.],&#10;         [7.],&#10;         [9.]],&#10;&#10;        [[5.],&#10;         [7.],&#10;         [5.],&#10;         [6.],&#10;         [9.]]],&#10;&#10;&#10;       [[[9.],&#10;         [8.],&#10;         [6.],&#10;         [4.],&#10;         [5.]],&#10;&#10;        [[9.],&#10;         [5.],&#10;         [8.],&#10;         [4.],&#10;         [5.]],&#10;&#10;        [[6.],&#10;         [7.],&#10;         [8.],&#10;         [5.],&#10;         [7.]],&#10;&#10;        [[8.],&#10;         [8.],&#10;         [4.],&#10;         [4.],&#10;         [6.]]],&#10;&#10;&#10;       [[[8.],&#10;         [4.],&#10;         [9.],&#10;         [6.],&#10;         [4.]],&#10;&#10;        [[6.],&#10;         [8.],&#10;         [7.],&#10;         [4.],&#10;         [9.]],&#10;&#10;        [[4.],&#10;         [7.],&#10;         [4.],&#10;         [5.],&#10;         [7.]],&#10;&#10;        [[9.],&#10;         [8.],&#10;         [1.],&#10;         [9.],&#10;         [9.]]]], dtype=float32)}.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec342dd0&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'op_name': 'POW', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryc4ubndz5'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec342dd0&gt;
framework_model = node {
  name: "Input_0"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
       ...0"
  input: "Input_1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryc4ubndz5', use_old_api = False
use_new_frontend = True, infer_timeout = 60, enabled_transforms = '', disabled_transforms = ''
kwargs = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'op_name': 'POW', 'shape': [3, 4, 5, 1]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite', compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...', 'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryc4ubndz5', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[[[8.235430e+05],
E                [1.000000e+00],
E                [7.290000e+02],
E                [3.430000e+02],
E                [7.812500e+04]],
E       
E               [[3.276800e+04],
E                [1.280000e+02],
E                [1.000000e+00],
E                [4.096000e+03],
E                [2.560000e+02]],
E       
E               [[3.000000e+00],
E                [1.000000e+00],
E                [5.120000e+02],
E                [8.235430e+05],
E                [6.561000e+03]],
E       
E               [[2.500000e+01],
E                [1.680700e+04],
E                [3.125000e+03],
E                [1.000000e+00],
E                [1.968300e+04]]],
E       
E       
E              [[[2.621440e+05],
E                [1.679616e+06],
E                [6.400000e+01],
E                [2.560000e+02],
E                [1.024000e+03]],
E       
E               [[2.621440e+05],
E                [1.000000e+00],
E                [1.679616e+06],
E                [1.600000e+01],
E                [3.200000e+01]],
E       
E               [[4.665600e+04],
E                [4.900000e+01],
E                [6.553600e+04],
E                [3.125000e+03],
E                [1.280000e+02]],
E       
E               [[5.764801e+06],
E                [5.120000e+02],
E                [1.600000e+01],
E                [1.600000e+01],
E                [1.000000e+00]]],
E       
E       
E              [[[2.560000e+02],
E                [2.560000e+02],
E                [1.000000e+00],
E                [1.562500e+04],
E                [4.000000e+00]],
E       
E               [[4.665600e+04],
E                [1.000000e+00],
E                [1.176490e+05],
E                [8.100000e+01],
E                [1.968300e+04]],
E       
E               [[1.600000e+01],
E                [7.000000e+00],
E                [1.600000e+01],
E                [5.000000e+00],
E                [2.799360e+05]],
E       
E               [[2.621440e+05],
E                [2.097152e+06],
E                [1.000000e+00],
E                [8.100000e+01],
E                [4.782969e+06]]]], dtype=float32)}; framework_res={'BinaryOperation': array([[[[7.],
E                [3.],
E                [9.],
E                [7.],
E                [7.]],
E       
E               [[8.],
E                [7.],
E                [5.],
E                [6.],
E                [4.]],
E       
E               [[3.],
E                [4.],
E                [8.],
E                [7.],
E                [9.]],
E       
E               [[5.],
E                [7.],
E                [5.],
E                [6.],
E                [9.]]],
E       
E       
E              [[[9.],
E                [8.],
E                [6.],
E                [4.],
E                [5.]],
E       
E               [[9.],
E                [5.],
E                [8.],
E                [4.],
E                [5.]],
E       
E               [[6.],
E                [7.],
E                [8.],
E                [5.],
E                [7.]],
E       
E               [[8.],
E                [8.],
E                [4.],
E                [4.],
E                [6.]]],
E       
E       
E              [[[8.],
E                [4.],
E                [9.],
E                [6.],
E                [4.]],
E       
E               [[6.],
E                [8.],
E                [7.],
E                [4.],
E                [9.]],
E       
E               [[4.],
E                [7.],
E                [4.],
E                [5.],
E                [7.]],
E       
E               [[9.],
E                [8.],
E                [1.],
E                [9.],
E                [9.]]]], dtype=float32)}.

../../common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7fc82b79b250&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.888" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.682"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502739a0&gt;
params = {'op_func': &lt;function maximum at 0x7f7d5229d990&gt;, 'op_name': 'MAXIMUM', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary7y3v6eft'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d4c1061a0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f3acd7c1e10&gt;, 'shape': [6, 7]} ]" time="0.739"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (FLOOR_MOD) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5a0e0&gt;
params = {'op_func': &lt;function equal at 0x7f3acd7c1e10&gt;, 'op_name': 'EQUAL', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryy4jo3l3z'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f3aa83eb940&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (FLOOR_MOD) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f295b7f9990&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.819"><failure message="ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cda50&gt;
params = {'op_func': &lt;function maximum at 0x7f295b7f9990&gt;, 'op_name': 'MAXIMUM', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryogrk14_i'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:101: in get_tflite_results
    interpreter.set_tensor(input_details[tensor_id]['index'], data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f29596f4cd0&gt;, tensor_index = 0
value = array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
         [  7....       [ -5.]],

        [[-10.],
         [  3.],
         [  2.],
         [ -4.],
         [ -1.]]]], dtype=float32)

    def set_tensor(self, tensor_index, value):
      """Sets the value of the input tensor.
    
      Note this copies data in `value`.
    
      If you want to avoid copying, you can use the `tensor()` function to get a
      numpy buffer pointing to the input buffer in the tflite interpreter.
    
      Args:
        tensor_index: Tensor index of tensor to set. This value can be gotten from
          the 'index' field in get_input_details.
        value: Value of tensor to set.
    
      Raises:
        ValueError: If the interpreter could not set the tensor.
      """
&gt;     self._interpreter.SetTensor(tensor_index, value)
E     ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type BOOL for input 0, name: Input_0

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7f06082270a0&gt;, 'shape': [6, 7]} ]" time="0.780"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LOGICAL_OR) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec342f80&gt;
params = {'op_func': &lt;function squared_difference at 0x7f06082270a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [6, 7]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryojvfdrb7'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f05e47b0520&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (LOGICAL_OR) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7fc8117a9870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.580"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7fc80f7ac0a0&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7fc8117a9870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [3, 4, 5, 1]}
ie_device = 'GPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryqxpkz5_z'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fc878faceb0&gt;&gt;
func_args = [{'Input_0': array([[[[7.],
         [1.],
         [9.],
         [7.],
         [5.]],

        [[8.],
         [2.]...,
         [7.]],

        [[9.],
         [7.],
         [1.],
         [2.],
         [7.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7f7d6c2b5ea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.698"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50273880&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7f7d6c2b5ea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryeh8kfsf0'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d4c1036a0&gt;&gt;
func_args = [{'Input_0': array([[[[False],
         [False],
         [False],
         [False],
         [False]],

        [[Fal...lse],
         [False],
         [False],
         [False],
         [False]]]])}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f3acdb96f80&gt;, 'shape': [6, 7]} ]" time="0.485"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5b400&gt;
params = {'op_func': &lt;function greater at 0x7f3acdb96f80&gt;, 'op_name': 'GREATER', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary2r67qcrv'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3b14fc9de0&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f297538f6d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="1.054"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cd270&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7f297538f6d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarymo9v5yk8'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f29bcbae1d0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7fc8117a9870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="0.006"><error message="failed on setup with &quot;worker 'gw2' crashed while running &quot;tensorflow_lite_tests/dummy/test_tfl_Binary.py::TestTFLiteBinaryLayerTest::test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7fc8117a9870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]&quot;&quot;">worker 'gw2' crashed while running "tensorflow_lite_tests/dummy/test_tfl_Binary.py::TestTFLiteBinaryLayerTest::test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7fc8117a9870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]"</error></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7f7d6c1170a0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.738"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502c4e20&gt;
params = {'op_func': &lt;function squared_difference at 0x7f7d6c1170a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary_91aoj2t'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7db35c8cd0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'LESS', 'op_func': &lt;function less at 0x7f06081ccc10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.601"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec3905b0&gt;
params = {'op_func': &lt;function less at 0x7f06081ccc10&gt;, 'op_name': 'LESS', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryazsv4ouc'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05e4715060&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'EQUAL', 'op_func': &lt;function equal at 0x7f3acd7c1e10&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.689"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5b010&gt;
params = {'op_func': &lt;function equal at 0x7f3acd7c1e10&gt;, 'op_name': 'EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryfzb062d2'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3aa83f3460&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f06083a2f80&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.515"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec343370&gt;
params = {'op_func': &lt;function greater at 0x7f06083a2f80&gt;, 'op_name': 'GREATER', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryc0jw6nu1'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05e4714a00&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'MINIMUM', 'op_func': &lt;function minimum at 0x7f7d5229da20&gt;, 'shape': [6, 7]} ]" time="0.797"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (SQUARED_DIFFERENCE) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50272b90&gt;
params = {'op_func': &lt;function minimum at 0x7f7d5229da20&gt;, 'op_name': 'MINIMUM', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarys8wn2qp8'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7db352ccd0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (SQUARED_DIFFERENCE) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7f29757ada20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.817"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (SQUARED_DIFFERENCE) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597ce7d0&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7f29757ada20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'CPU', precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryrn77rn1_'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f29bcb2d600&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (SQUARED_DIFFERENCE) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7f3acda1b0a0&gt;, 'shape': [6, 7]} ]" time="0.831"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (FLOOR_MOD) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5af80&gt;
params = {'op_func': &lt;function squared_difference at 0x7f3acda1b0a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [6, 7]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryfvnhbtrh'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f3aa83ebdc0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (FLOOR_MOD) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7f05ee365870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.925"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (MAXIMUM) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec3411b0&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7f05ee365870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryp1oo34i7'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f064f5eee30&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (MAXIMUM) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LOGICAL_OR', 'op_func': &lt;function logical_or at 0x7f7d6c2b5ea0&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [6, 7]} ]" time="0.741"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50273910&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_or at 0x7f7d6c2b5ea0&gt;, 'op_name': 'LOGICAL_OR', ...}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarybqzoetp0'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7d4c101930&gt;&gt;
func_args = [{'Input_0': array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, Fals...False, False, False],
       [False, False, False, False, False, False, False]])}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'MAXIMUM', 'op_func': &lt;function maximum at 0x7f295b7f9990&gt;, 'shape': [6, 7]} ]" time="0.832"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (GREATER) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cdae0&gt;
params = {'op_func': &lt;function maximum at 0x7f295b7f9990&gt;, 'op_name': 'MAXIMUM', 'shape': [6, 7]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryudwf_vur'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f29bcbda680&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (GREATER) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f3acdb96f80&gt;, 'shape': [3, 4, 5, 1]} ]" time="1.018" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7f06081ccee0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.838"><failure message="TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec341750&gt;
params = {'op_func': &lt;function less_equal at 0x7f06081ccee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary8vt_67gu'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
../../common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([[[[ True],
         [ True],
         [ True],
         [False],
         [ True]],

        [[ True],
        ...True],
         [ True]],

        [[ True],
         [False],
         [False],
         [False],
         [ True]]]])
ref_array = array([[[[ -1.],
         [  0.],
         [-10.],
         [ -7.],
         [  2.]],

        [[ -6.],
         [  2....       [ -5.]],

        [[-10.],
         [  1.],
         [  1.],
         [-10.],
         [ -1.]]]], dtype=float32)
atol = 0.0001, rtol = 0.0001

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
&gt;           abs_diff = np.absolute(cur_array ^ ref_array)
E           TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

../../common/utils/common_utils.py:100: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f7d6c292f80&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.567"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502732e0&gt;
params = {'op_func': &lt;function greater at 0x7f7d6c292f80&gt;, 'op_name': 'GREATER', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binarylcyjkw3x'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7db35b8640&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'MINIMUM', 'op_func': &lt;function minimum at 0x7f295b7f9a20&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.660"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cfaf0&gt;
params = {'op_func': &lt;function minimum at 0x7f295b7f9a20&gt;, 'op_name': 'MINIMUM', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary4u4j9lle'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f29bcbe3b50&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'LESS_EQUAL', 'op_func': &lt;function less_equal at 0x7f3acdbbcee0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.801"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (GREATER) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5a710&gt;
params = {'op_func': &lt;function less_equal at 0x7f3acdbbcee0&gt;, 'op_name': 'LESS_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarytc6stl1f'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f3b14fcd990&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (GREATER) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'NOT_EQUAL', 'op_func': &lt;function not_equal at 0x7f7d6beb9fc0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.947"><failure message="TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50271c60&gt;
params = {'op_func': &lt;function not_equal at 0x7f7d6beb9fc0&gt;, 'op_name': 'NOT_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryi5m8ivri'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
../../common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([[False,  True, False, False,  True,  True,  True],
       [False,  True, False,  True, False, False,  True],
  ...],
       [False,  True, False,  True, False,  True,  True],
       [False,  True,  True, False, False,  True, False]])
ref_array = array([[ 6.,  0.,  2., -0., -7., -6., -3.],
       [-0., nan, -0., -3., -1., -0.,  0.],
       [ 0.,  1., -3.,  0.,  2... 0.,  1., -2.],
       [ 2., -1.,  1., -2.,  7., -1., -1.],
       [nan, -2., -6.,  2.,  3., -4.,  5.]], dtype=float32)
atol = 0.0001, rtol = 0.0001

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
&gt;           abs_diff = np.absolute(cur_array ^ ref_array)
E           TypeError: ufunc 'bitwise_xor' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

../../common/utils/common_utils.py:100: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f06083a2f80&gt;, 'shape': [6, 7]} ]" time="0.513"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec390400&gt;
params = {'op_func': &lt;function greater at 0x7f06083a2f80&gt;, 'op_name': 'GREATER', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryrpp4lw74'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f064f5dcd30&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_MOD', 'op_func': &lt;function floor_mod at 0x7f295b7f9870&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="1.552"><failure message="RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (NOT_EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f29597cd1e0&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floor_mod at 0x7f295b7f9870&gt;, 'op_name': 'FLOOR_MOD', 'shape': [6, 7]}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binarykenqk4cc'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f2959f52ec0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [6,7] and [3,4,5,1], are not broadcastable.Node number 0 (NOT_EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'LOGICAL_AND', 'op_func': &lt;function logical_and at 0x7f3acdbbda20&gt;, 'kwargs_to_prepare_input': 'boolean', 'dtype': tf.bool, 'shape': [3, 4, 5, 1]} ]" time="0.668"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b5b7f0&gt;
params = {'dtype': tf.bool, 'kwargs_to_prepare_input': 'boolean', 'op_func': &lt;function logical_and at 0x7f3acdbbda20&gt;, 'op_name': 'LOGICAL_AND', ...}
ie_device = 'GPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryvtnz31pk'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3ab1a800a0&gt;&gt;
func_args = [{'Input_0': array([[False, False, False, False, False, False, False],
       [False, False, False, False, False, Fals...False, False, False],
       [False, False, False, False, False, False, False]])}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP16 - params:{'op_name': 'POW', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [6, 7]} ]" time="0.779"><failure message="AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[7.000000e+00, 1.000000e+00, 6.561000e+03, 1.176490e+05,&#10;        5.000000e+00, 2.097152e+06, 8.000000e+00],&#10;       [1.000000e+00, 2.560000e+02, 1.638400e+04, 8.100000e+01,&#10;        1.000000e+00, 2.621440e+05, 2.401000e+03],&#10;       [4.304672e+07, 5.000000e+00, 4.035361e+07, 1.953125e+06,&#10;        1.000000e+00, 2.700000e+01, 6.400000e+01],&#10;       [2.160000e+02, 1.280000e+02, 1.024000e+03, 1.638400e+04,&#10;        1.024000e+03, 1.000000e+00, 1.296000e+03],&#10;       [2.000000e+00, 1.600000e+01, 2.160000e+02, 8.235430e+05,&#10;        2.560000e+02, 2.500000e+01, 3.200000e+01],&#10;       [1.680700e+04, 2.621440e+05, 5.120000e+02, 5.120000e+02,&#10;        1.000000e+00, 6.400000e+01, 2.560000e+02]], dtype=float32)}; framework_res={'BinaryOperation': array([[ True, False,  True,  True,  True,  True, False],&#10;       [False, False, False, False, False,  True,  True],&#10;       [ True,  True, False, False, False, False,  True],&#10;       [ True, False, False, False, False, False,  True],&#10;       [ True, False,  True, False, False,  True, False],&#10;       [ True,  True, False, False, False, False, False]])}.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec342e60&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'op_name': 'POW', 'shape': [6, 7]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryeokq8_la'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec342e60&gt;
framework_model = node {
  name: "Input_0"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
       ...0"
  input: "Input_1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryeokq8_la', use_old_api = False
use_new_frontend = True, infer_timeout = 60, enabled_transforms = '', disabled_transforms = ''
kwargs = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function pow at 0x7f0607fa8ca0&gt;, 'op_name': 'POW', 'shape': [6, 7]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite', compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...', 'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binaryeokq8_la', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'BinaryOperation': array([[7.000000e+00, 1.000000e+00, 6.561000e+03, 1.176490e+05,
E               5.000000e+00, 2.097152e+06, 8.000000e+00],
E              [1.000000e+00, 2.560000e+02, 1.638400e+04, 8.100000e+01,
E               1.000000e+00, 2.621440e+05, 2.401000e+03],
E              [4.304672e+07, 5.000000e+00, 4.035361e+07, 1.953125e+06,
E               1.000000e+00, 2.700000e+01, 6.400000e+01],
E              [2.160000e+02, 1.280000e+02, 1.024000e+03, 1.638400e+04,
E               1.024000e+03, 1.000000e+00, 1.296000e+03],
E              [2.000000e+00, 1.600000e+01, 2.160000e+02, 8.235430e+05,
E               2.560000e+02, 2.500000e+01, 3.200000e+01],
E              [1.680700e+04, 2.621440e+05, 5.120000e+02, 5.120000e+02,
E               1.000000e+00, 6.400000e+01, 2.560000e+02]], dtype=float32)}; framework_res={'BinaryOperation': array([[ True, False,  True,  True,  True,  True, False],
E              [False, False, False, False, False,  True,  True],
E              [ True,  True, False, False, False, False,  True],
E              [ True, False, False, False, False, False,  True],
E              [ True, False,  True, False, False,  True, False],
E              [ True,  True, False, False, False, False, False]])}.

../../common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'GREATER', 'op_func': &lt;function greater at 0x7f7d6c292f80&gt;, 'shape': [6, 7]} ]" time="0.620"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50273370&gt;
params = {'op_func': &lt;function greater at 0x7f7d6c292f80&gt;, 'op_name': 'GREATER', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary6b2uwah3'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7db352c940&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ...  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'NOT_EQUAL', 'op_func': &lt;function not_equal at 0x7f3acd7c1fc0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.823"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (GREATER_EQUAL) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1b59cf0&gt;
params = {'op_func': &lt;function not_equal at 0x7f3acd7c1fc0&gt;, 'op_name': 'NOT_EQUAL', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary_2xifjdg'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f3b14fd9930&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (GREATER_EQUAL) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP32 - params:{'op_name': 'MINIMUM', 'op_func': &lt;function minimum at 0x7f05ee365a20&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.559"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec343b50&gt;
params = {'op_func': &lt;function minimum at 0x7f05ee365a20&gt;, 'op_name': 'MINIMUM', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryabbu6yff'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f05e47ac8e0&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...    [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'MINIMUM', 'op_func': &lt;function minimum at 0x7f7d5229da20&gt;, 'shape': [6, 7]} ]" time="0.808"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d502c4b50&gt;
params = {'op_func': &lt;function minimum at 0x7f7d5229da20&gt;, 'op_name': 'MINIMUM', 'shape': [6, 7]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binary8e3ho7p4'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7db35bb460&gt;&gt;
func_args = [{'Input_0': array([[ -1.,   0., -10.,  -2.,   2.,  -6.,   7.],
       [-10.,   5.,  -7.,   9.,  -8., -10.,   0.],
   ... [  3., -10.,   8.,  -8.,   8.,  -4.,  -4.],
       [  0.,  -5.,  -7.,   5.,   6.,  -7.,   8.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f2975787250&gt;, 'shape': [6, 7]} ]" time="0.021"><error message="failed on setup with &quot;worker 'gw0' crashed while running &quot;tensorflow_lite_tests/dummy/test_tfl_Binary.py::TestTFLiteBinaryLayerTest::test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f2975787250&gt;, 'shape': [6, 7]} ]&quot;&quot;">worker 'gw0' crashed while running "tensorflow_lite_tests/dummy/test_tfl_Binary.py::TestTFLiteBinaryLayerTest::test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'GREATER_EQUAL', 'op_func': &lt;function greater_equal at 0x7f2975787250&gt;, 'shape': [6, 7]} ]"</error></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'MINIMUM', 'op_func': &lt;function minimum at 0x7f05ee365a20&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.801"><failure message="RuntimeError: Division by 0Node number 0 (FLOOR_DIV) failed to invoke.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f05ec341bd0&gt;
params = {'op_func': &lt;function minimum at 0x7f05ee365a20&gt;, 'op_name': 'MINIMUM', 'shape': [3, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary9emj9lq8'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:103: in get_tflite_results
    interpreter.invoke()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f05e47e95d0&gt;

    def invoke(self):
      """Invoke the interpreter.
    
      Be sure to set the input sizes, allocate tensors and fill values before
      calling this. Also, note that this function releases the GIL so heavy
      computation can be done in the background while the Python interpreter
      continues. No other function on this object should be called while the
      invoke() call has not finished.
    
      Raises:
        ValueError: When the underlying interpreter fails raise ValueError.
      """
      self._ensure_safe()
&gt;     self._interpreter.Invoke()
E     RuntimeError: Division by 0Node number 0 (FLOOR_DIV) failed to invoke.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:941: RuntimeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:GPU - precision:FP16 - params:{'op_name': 'SQUARED_DIFFERENCE', 'op_func': &lt;function squared_difference at 0x7f3acda1b0a0&gt;, 'shape': [3, 4, 5, 1]} ]" time="0.779"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f3ab1ba8eb0&gt;
params = {'op_func': &lt;function squared_difference at 0x7f3acda1b0a0&gt;, 'op_name': 'SQUARED_DIFFERENCE', 'shape': [3, 4, 5, 1]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_binaryw3jzfye5'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
../../common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3b14fb7f40&gt;&gt;
func_args = [{'Input_0': array([[[[ -1.],
         [  0.],
         [-10.],
         [ -2.],
         [  2.]],

        [[ -6.],
 ...  9.]],

        [[ -5.],
         [  1.],
         [  1.],
         [-10.],
         [  4.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

../../common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_Binary.TestTFLiteBinaryLayerTest" name="test_binary[ ie_device:CPU - precision:FP32 - params:{'op_name': 'FLOOR_DIV', 'op_func': &lt;function floordiv at 0x7f7d6be9b6d0&gt;, 'kwargs_to_prepare_input': 'positive', 'shape': [3, 4, 5, 1]} ]" time="0.890"><failure message="RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_OR) failed to prepare.">self = &lt;test_tfl_Binary.TestTFLiteBinaryLayerTest object at 0x7f7d50271240&gt;
params = {'kwargs_to_prepare_input': 'positive', 'op_func': &lt;function floordiv at 0x7f7d6be9b6d0&gt;, 'op_name': 'FLOOR_DIV', 'shape': [3, 4, 5, 1]}
ie_device = 'CPU', precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_binary9y8qam2f'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    @pytest.mark.xfail(platform.machine() in ["aarch64", "arm64", "ARM64"],
                       reason='Ticket - 123324')
    def test_binary(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

test_tfl_Binary.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
../../common/layer_test_class.py:108: in _test
    fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
../../common/tflite_layer_test_class.py:47: in get_framework_results
    return get_tflite_results(self.use_new_frontend, self.use_old_api, inputs_dict, model_path)
../../common/utils/tflite_utils.py:99: in get_tflite_results
    interpreter.allocate_tensors()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;tensorflow.lite.python.interpreter.Interpreter object at 0x7f7d4c104af0&gt;

    def allocate_tensors(self):
      self._ensure_safe()
&gt;     return self._interpreter.AllocateTensors()
E     RuntimeError: Given shapes, [3,4,5,1] and [6,7], are not broadcastable.Node number 0 (LOGICAL_OR) failed to prepare.

/home/chaitanyasai/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:531: RuntimeError</failure></testcase><testcase time="0.030" /><testcase time="0.008" /><testcase time="0.006" /><testcase classname="pytest" name="internal" time="0.000"><error message="internal error">Traceback (most recent call last):
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/_pytest/main.py", line 271, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/_pytest/main.py", line 325, in _main
    config.hook.pytest_runtestloop(session=session)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_hooks.py", line 493, in __call__
    return self._hookexec(self.name, self._hookimpls, kwargs, firstresult)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_manager.py", line 115, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_callers.py", line 152, in _multicall
    return outcome.get_result()
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_result.py", line 114, in get_result
    raise exc.with_traceback(exc.__traceback__)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_callers.py", line 77, in _multicall
    res = hook_impl.function(*args)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/xdist/dsession.py", line 123, in pytest_runtestloop
    self.loop_once()
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/xdist/dsession.py", line 148, in loop_once
    call(**kwargs)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/xdist/dsession.py", line 273, in worker_collectionfinish
    self.sched.schedule()
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/xdist/scheduler/each.py", line 134, in schedule
    pending[:] = range(len(self.node2collection[node]))
KeyError: &lt;WorkerController gw7&gt;</error></testcase></testsuite></testsuites>
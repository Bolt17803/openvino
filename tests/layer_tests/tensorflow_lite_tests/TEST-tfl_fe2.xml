<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="2" failures="41" skipped="0" tests="54" time="38.559" timestamp="2024-01-25T17:01:34.979495" hostname="chaitanyasai-HP-ProBook-440-G5"><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 1, 'shape': [1, 3]} ]" time="0.115"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff430&gt;, params = {'num_inputs': 1, 'shape': [1, 3]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_n7hku18h6'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff430&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 1, 'shape': [1, 3]} ]" time="0.111"><failure message="struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43430&gt;, params = {'num_inputs': 1, 'shape': [1, 3]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_n1jkwzxm_'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
tests/layer_tests/common/tflite_layer_test_class.py:54: in check_tflite_model_has_only_allowed_ops
    model = utils.read_model(self.model_path)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:61: in read_model
    model = convert_bytearray_to_object(model_bytearray)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:40: in convert_bytearray_to_object
    model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17784: in GetRootAsModel
    return cls.GetRootAs(buf, offset)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17776: in GetRootAs
    n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

packer_type = &lt;_struct.Struct object at 0x7f3404bd98a0&gt;, buf = bytearray(b''), head = 0

    def Get(packer_type, buf, head):
        """ Get decodes a value at buf[head] using `packer_type`. """
&gt;       return packer_type.unpack_from(memoryview_type(buf), head)[0]
E       struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)

../../../.local/lib/python3.10/site-packages/flatbuffers/encode.py:26: error</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 5, 'shape': [2, 8, 5, 2]} ]" time="1.923" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 1, 'shape': [1, 3]} ]" time="0.084"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff0d0&gt;, params = {'num_inputs': 1, 'shape': [1, 3]}, ie_device = 'CPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_n7k1znwf_'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff0d0&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 4, 'shape': [2, 8, 5, 2]} ]" time="0.407"><failure message="AssertionError: Comparing with Framework failed: ie_res={'AddN': array([[[[ 20.,   3.],&#10;         [ 18.,   6.],&#10;         [ -5.,   5.],&#10;         [ -1.,   1.],&#10;         [ -6.,  -1.]],&#10;&#10;        [[ -4., -10.],&#10;         [ 12., -18.],&#10;         [ 17.,  -7.],&#10;         [ -2., -17.],&#10;         [ -7., -18.]],&#10;&#10;        [[  0., -17.],&#10;         [-18.,  -3.],&#10;         [  5.,   0.],&#10;         [ -1.,  -2.],&#10;         [-18., -12.]],&#10;&#10;        [[-29.,   8.],&#10;         [-16., -11.],&#10;         [ -4.,  -8.],&#10;         [-16.,  21.],&#10;         [ -3.,   8.]],&#10;&#10;        [[ -5., -20.],&#10;         [-15., -11.],&#10;         [  4.,  -2.],&#10;         [ -9., -20.],&#10;         [  2.,  10.]],&#10;&#10;        [[  9.,  -8.],&#10;         [ -6.,  -2.],&#10;         [  4.,   5.],&#10;         [ -6.,  -2.],&#10;         [ 19., -20.]],&#10;&#10;        [[  9.,  -6.],&#10;         [ 14.,   5.],&#10;         [  3., -15.],&#10;         [ -6.,  -3.],&#10;         [ 14.,  -8.]],&#10;&#10;        [[ 11.,   4.],&#10;         [ 16.,  -9.],&#10;         [ 14.,   0.],&#10;         [ 25.,  -6.],&#10;         [ -3.,  11.]]],&#10;&#10;&#10;       [[[-29., -14.],&#10;         [  7.,   3.],&#10;         [ -3.,   6.],&#10;         [ -6.,   1.],&#10;         [ 10.,  -8.]],&#10;&#10;        [[ -2.,  11.],&#10;         [  4.,  -5.],&#10;         [ 11.,  -7.],&#10;         [ 16.,  13.],&#10;         [ 12.,  -5.]],&#10;&#10;        [[ -1.,   4.],&#10;         [-22.,  -7.],&#10;         [ -6., -25.],&#10;         [  4.,   0.],&#10;         [  6.,   8.]],&#10;&#10;        [[ 10.,  -7.],&#10;         [  9.,   6.],&#10;         [  5.,  -3.],&#10;         [ 10., -23.],&#10;         [-15.,   3.]],&#10;&#10;        [[  2.,   9.],&#10;         [-16.,  -6.],&#10;         [ -7., -13.],&#10;         [ 10.,   0.],&#10;         [ -1., -20.]],&#10;&#10;        [[ 13., -22.],&#10;         [ 10.,  -3.],&#10;         [-20., -15.],&#10;         [ -6.,   0.],&#10;         [-10.,   7.]],&#10;&#10;        [[  7.,  -1.],&#10;         [ 11.,   2.],&#10;         [-10.,   4.],&#10;         [-11.,  -7.],&#10;         [ -9.,   6.]],&#10;&#10;        [[-14.,  14.],&#10;         [ 14.,  17.],&#10;         [ -3.,   2.],&#10;         [ -4.,  -7.],&#10;         [-17., -13.]]]], dtype=float32)}; framework_res={'AddN': array([[[[ 2.0000000e+01,  3.0000000e+00],&#10;         [ 1.8000000e+01,  6.0000000e+00],&#10;         [ 4.2641785e+34,  5.0000000e+00],&#10;         [-1.0000000e+00,  1.0000000e+00],&#10;         [-6.0000000e+00, -1.0000000e+00]],&#10;&#10;        [[-4.0000000e+00, -1.0000000e+01],&#10;         [ 1.2000000e+01, -1.8000000e+01],&#10;         [ 1.7253014e+04, -7.0000000e+00],&#10;         [ 1.7231367e+04, -1.7000000e+01],&#10;         [ 2.2521000e+04, -1.8000000e+01]],&#10;&#10;        [[ 1.7505300e+19, -1.7000000e+01],&#10;         [-1.8000000e+01, -3.0000000e+00],&#10;         [ 5.0000000e+00,  3.0862197e-41],&#10;         [-1.0000000e+00, -2.0000000e+00],&#10;         [-1.8000000e+01, -1.2000000e+01]],&#10;&#10;        [[-2.9000000e+01,  8.0000000e+00],&#10;         [-1.6000000e+01, -1.1000000e+01],&#10;         [-4.0000000e+00, -8.0000000e+00],&#10;         [ 4.2386354e+34,  2.1000000e+01],&#10;         [-3.0000000e+00,  8.0000000e+00]],&#10;&#10;        [[-5.0000000e+00, -2.0000000e+01],&#10;         [-1.5000000e+01, -1.1000000e+01],&#10;         [ 4.0000000e+00, -2.0000000e+00],&#10;         [ 1.7226727e+04, -2.0000000e+01],&#10;         [ 1.7236463e+04,  1.0000000e+01]],&#10;&#10;        [[ 2.2591406e+04, -8.0000000e+00],&#10;         [-6.0000000e+00, -2.0000000e+00],&#10;         [ 4.0000000e+00,  5.0000000e+00],&#10;         [-6.0000000e+00, -2.0000000e+00],&#10;         [ 1.9000000e+01, -2.0000000e+01]],&#10;&#10;        [[           nan, -6.0000000e+00],&#10;         [ 3.4061966e+12,  5.0000000e+00],&#10;         [ 3.0000000e+00, -1.5000000e+01],&#10;         [-6.0000000e+00, -3.0000000e+00],&#10;         [ 4.2384452e+34, -8.0000000e+00]],&#10;&#10;        [[ 1.1000000e+01,  4.0000000e+00],&#10;         [ 1.6000000e+01, -9.0000000e+00],&#10;         [ 1.4000000e+01,  3.0862197e-41],&#10;         [ 2.5000000e+01, -6.0000000e+00],&#10;         [ 1.7232883e+04,  1.1000000e+01]]],&#10;&#10;&#10;       [[[ 1.7206867e+04, -1.4000000e+01],&#10;         [ 2.2535000e+04,  3.0000000e+00],&#10;         [ 1.2175411e+13,  1.1337965e+21],&#10;         [-6.0000000e+00,  1.0000000e+00],&#10;         [ 1.0000000e+01, -8.0000000e+00]],&#10;&#10;        [[-2.0000000e+00,  1.1000000e+01],&#10;         [           nan, -5.0000000e+00],&#10;         [ 8.4441965e+08, -7.0000000e+00],&#10;         [ 1.6000000e+01,  1.3000000e+01],&#10;         [ 1.2000000e+01, -5.0000000e+00]],&#10;&#10;        [[ 4.2383185e+34,  4.0000000e+00],&#10;         [-2.2000000e+01, -7.0000000e+00],&#10;         [-6.0000000e+00, -2.5000000e+01],&#10;         [ 4.0000000e+00,  3.0862197e-41],&#10;         [ 6.0000000e+00,  8.0000000e+00]],&#10;&#10;        [[ 1.7245916e+04, -7.0000000e+00],&#10;         [ 1.7242367e+04,  6.0000000e+00],&#10;         [ 2.2586469e+04, -3.0000000e+00],&#10;         [ 1.0000000e+01, -2.3000000e+01],&#10;         [-1.5000000e+01,  3.0000000e+00]],&#10;&#10;        [[ 2.0000000e+00,  9.0000000e+00],&#10;         [-1.6000000e+01, -6.0000000e+00],&#10;         [           nan, -1.3000000e+01],&#10;         [ 9.0740071e+11,  1.3332341e+01],&#10;         [-1.0000000e+00, -2.0000000e+01]],&#10;&#10;        [[ 1.3000000e+01, -2.2000000e+01],&#10;         [ 4.2641152e+34, -3.0000000e+00],&#10;         [-2.0000000e+01, -1.5000000e+01],&#10;         [-6.0000000e+00,  0.0000000e+00],&#10;         [-1.0000000e+01,  7.0000000e+00]],&#10;&#10;        [[ 7.0000000e+00, -1.0000000e+00],&#10;         [ 1.7246936e+04,  2.0000000e+00],&#10;         [ 1.7223367e+04,  4.0000000e+00],&#10;         [ 2.2576094e+04, -7.0000000e+00],&#10;         [-9.0000000e+00,  5.1193279e+12]],&#10;&#10;        [[-1.4000000e+01,  1.4000000e+01],&#10;         [ 1.4000000e+01,  1.7000000e+01],&#10;         [ 5.9802174e+20,  2.0000000e+00],&#10;         [-4.0000000e+00, -7.0000000e+00],&#10;         [-1.7000000e+01, -1.3000000e+01]]]], dtype=float32)}.">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff4c0&gt;, params = {'num_inputs': 4, 'shape': [2, 8, 5, 2]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_no6ev8fxf'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff4c0&gt;
framework_model = node {
  name: "Input_0"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
       ...
      type: DT_FLOAT
    }
  }
  attr {
    key: "N"
    value {
      i: 4
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_no6ev8fxf', use_old_api = False
use_new_frontend = True, infer_timeout = 60, enabled_transforms = '', disabled_transforms = '', kwargs = {'num_inputs': 4, 'shape': [2, 8, 5, 2]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite', compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...l', 'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_no6ev8fxf', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'AddN': array([[[[ 20.,   3.],
E                [ 18.,   6.],
E                [ -5.,   5.],
E                [ -1.,   1.],
E                [ -6.,  -1.]],
E       
E               [[ -4., -10.],
E                [ 12., -18.],
E                [ 17.,  -7.],
E                [ -2., -17.],
E                [ -7., -18.]],
E       
E               [[  0., -17.],
E                [-18.,  -3.],
E                [  5.,   0.],
E                [ -1.,  -2.],
E                [-18., -12.]],
E       
E               [[-29.,   8.],
E                [-16., -11.],
E                [ -4.,  -8.],
E                [-16.,  21.],
E                [ -3.,   8.]],
E       
E               [[ -5., -20.],
E                [-15., -11.],
E                [  4.,  -2.],
E                [ -9., -20.],
E                [  2.,  10.]],
E       
E               [[  9.,  -8.],
E                [ -6.,  -2.],
E                [  4.,   5.],
E                [ -6.,  -2.],
E                [ 19., -20.]],
E       
E               [[  9.,  -6.],
E                [ 14.,   5.],
E                [  3., -15.],
E                [ -6.,  -3.],
E                [ 14.,  -8.]],
E       
E               [[ 11.,   4.],
E                [ 16.,  -9.],
E                [ 14.,   0.],
E                [ 25.,  -6.],
E                [ -3.,  11.]]],
E       
E       
E              [[[-29., -14.],
E                [  7.,   3.],
E                [ -3.,   6.],
E                [ -6.,   1.],
E                [ 10.,  -8.]],
E       
E               [[ -2.,  11.],
E                [  4.,  -5.],
E                [ 11.,  -7.],
E                [ 16.,  13.],
E                [ 12.,  -5.]],
E       
E               [[ -1.,   4.],
E                [-22.,  -7.],
E                [ -6., -25.],
E                [  4.,   0.],
E                [  6.,   8.]],
E       
E               [[ 10.,  -7.],
E                [  9.,   6.],
E                [  5.,  -3.],
E                [ 10., -23.],
E                [-15.,   3.]],
E       
E               [[  2.,   9.],
E                [-16.,  -6.],
E                [ -7., -13.],
E                [ 10.,   0.],
E                [ -1., -20.]],
E       
E               [[ 13., -22.],
E                [ 10.,  -3.],
E                [-20., -15.],
E                [ -6.,   0.],
E                [-10.,   7.]],
E       
E               [[  7.,  -1.],
E                [ 11.,   2.],
E                [-10.,   4.],
E                [-11.,  -7.],
E                [ -9.,   6.]],
E       
E               [[-14.,  14.],
E                [ 14.,  17.],
E                [ -3.,   2.],
E                [ -4.,  -7.],
E                [-17., -13.]]]], dtype=float32)}; framework_res={'AddN': array([[[[ 2.0000000e+01,  3.0000000e+00],
E                [ 1.8000000e+01,  6.0000000e+00],
E                [ 4.2641785e+34,  5.0000000e+00],
E                [-1.0000000e+00,  1.0000000e+00],
E                [-6.0000000e+00, -1.0000000e+00]],
E       
E               [[-4.0000000e+00, -1.0000000e+01],
E                [ 1.2000000e+01, -1.8000000e+01],
E                [ 1.7253014e+04, -7.0000000e+00],
E                [ 1.7231367e+04, -1.7000000e+01],
E                [ 2.2521000e+04, -1.8000000e+01]],
E       
E               [[ 1.7505300e+19, -1.7000000e+01],
E                [-1.8000000e+01, -3.0000000e+00],
E                [ 5.0000000e+00,  3.0862197e-41],
E                [-1.0000000e+00, -2.0000000e+00],
E                [-1.8000000e+01, -1.2000000e+01]],
E       
E               [[-2.9000000e+01,  8.0000000e+00],
E                [-1.6000000e+01, -1.1000000e+01],
E                [-4.0000000e+00, -8.0000000e+00],
E                [ 4.2386354e+34,  2.1000000e+01],
E                [-3.0000000e+00,  8.0000000e+00]],
E       
E               [[-5.0000000e+00, -2.0000000e+01],
E                [-1.5000000e+01, -1.1000000e+01],
E                [ 4.0000000e+00, -2.0000000e+00],
E                [ 1.7226727e+04, -2.0000000e+01],
E                [ 1.7236463e+04,  1.0000000e+01]],
E       
E               [[ 2.2591406e+04, -8.0000000e+00],
E                [-6.0000000e+00, -2.0000000e+00],
E                [ 4.0000000e+00,  5.0000000e+00],
E                [-6.0000000e+00, -2.0000000e+00],
E                [ 1.9000000e+01, -2.0000000e+01]],
E       
E               [[           nan, -6.0000000e+00],
E                [ 3.4061966e+12,  5.0000000e+00],
E                [ 3.0000000e+00, -1.5000000e+01],
E                [-6.0000000e+00, -3.0000000e+00],
E                [ 4.2384452e+34, -8.0000000e+00]],
E       
E               [[ 1.1000000e+01,  4.0000000e+00],
E                [ 1.6000000e+01, -9.0000000e+00],
E                [ 1.4000000e+01,  3.0862197e-41],
E                [ 2.5000000e+01, -6.0000000e+00],
E                [ 1.7232883e+04,  1.1000000e+01]]],
E       
E       
E              [[[ 1.7206867e+04, -1.4000000e+01],
E                [ 2.2535000e+04,  3.0000000e+00],
E                [ 1.2175411e+13,  1.1337965e+21],
E                [-6.0000000e+00,  1.0000000e+00],
E                [ 1.0000000e+01, -8.0000000e+00]],
E       
E               [[-2.0000000e+00,  1.1000000e+01],
E                [           nan, -5.0000000e+00],
E                [ 8.4441965e+08, -7.0000000e+00],
E                [ 1.6000000e+01,  1.3000000e+01],
E                [ 1.2000000e+01, -5.0000000e+00]],
E       
E               [[ 4.2383185e+34,  4.0000000e+00],
E                [-2.2000000e+01, -7.0000000e+00],
E                [-6.0000000e+00, -2.5000000e+01],
E                [ 4.0000000e+00,  3.0862197e-41],
E                [ 6.0000000e+00,  8.0000000e+00]],
E       
E               [[ 1.7245916e+04, -7.0000000e+00],
E                [ 1.7242367e+04,  6.0000000e+00],
E                [ 2.2586469e+04, -3.0000000e+00],
E                [ 1.0000000e+01, -2.3000000e+01],
E                [-1.5000000e+01,  3.0000000e+00]],
E       
E               [[ 2.0000000e+00,  9.0000000e+00],
E                [-1.6000000e+01, -6.0000000e+00],
E                [           nan, -1.3000000e+01],
E                [ 9.0740071e+11,  1.3332341e+01],
E                [-1.0000000e+00, -2.0000000e+01]],
E       
E               [[ 1.3000000e+01, -2.2000000e+01],
E                [ 4.2641152e+34, -3.0000000e+00],
E                [-2.0000000e+01, -1.5000000e+01],
E                [-6.0000000e+00,  0.0000000e+00],
E                [-1.0000000e+01,  7.0000000e+00]],
E       
E               [[ 7.0000000e+00, -1.0000000e+00],
E                [ 1.7246936e+04,  2.0000000e+00],
E                [ 1.7223367e+04,  4.0000000e+00],
E                [ 2.2576094e+04, -7.0000000e+00],
E                [-9.0000000e+00,  5.1193279e+12]],
E       
E               [[-1.4000000e+01,  1.4000000e+01],
E                [ 1.4000000e+01,  1.7000000e+01],
E                [ 5.9802174e+20,  2.0000000e+00],
E                [-4.0000000e+00, -7.0000000e+00],
E                [-1.7000000e+01, -1.3000000e+01]]]], dtype=float32)}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 5, 'shape': [2, 8, 5, 2]} ]" time="2.330" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'num_inputs': 4, 'shape': [1, 3]} ]" time="0.373"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff8b0&gt;, params = {'num_inputs': 4, 'shape': [1, 3]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_norg1elsb'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7cf428b6d0&gt;&gt;
func_args = [{'Input_0': array([[7., 7., 2.]], dtype=float32), 'Input_1': array([[ 3., -2.,  7.]], dtype=float32), 'Input_2': arra...9., -4.]], dtype=float32), 'Input_3': array([[-10.,   0., -10.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 4, 'shape': [1, 3]} ]" time="0.412" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'num_inputs': 1, 'shape': [2, 8, 5, 2]} ]" time="0.120"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ffa60&gt;, params = {'num_inputs': 1, 'shape': [2, 8, 5, 2]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n1xn3kt2p'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ffa60&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'num_inputs': 5, 'shape': [1, 3]} ]" time="0.250"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff9d0&gt;, params = {'num_inputs': 5, 'shape': [1, 3]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n9j6_oggs'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f7cf42a1780&gt;&gt;
func_args = [{'Input_0': array([[7., 7., 2.]], dtype=float32), 'Input_1': array([[ 3., -2.,  7.]], dtype=float32), 'Input_2': arra...4.]], dtype=float32), 'Input_3': array([[-10.,   0., -10.]], dtype=float32), ...}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 4, 'shape': [1, 3]} ]" time="0.473" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 1, 'shape': [2, 8, 5, 2]} ]" time="0.128"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff3a0&gt;, params = {'num_inputs': 1, 'shape': [2, 8, 5, 2]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_nknus1n0s'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f7cf55ff3a0&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 1, 'shape': [1, 3]} ]" time="0.110"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e430d0&gt;, params = {'num_inputs': 1, 'shape': [1, 3]}, ie_device = 'CPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_ndmppvrmi'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e430d0&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 5, 'shape': [1, 3]} ]" time="0.004"><error message="failed on setup with &quot;worker 'gw0' crashed while running &quot;tensorflow_lite_tests/dummy/test_tfl_AddN.py::TestTFLiteAddNLayerTest::test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 5, 'shape': [1, 3]} ]&quot;&quot;">worker 'gw0' crashed while running "tensorflow_lite_tests/dummy/test_tfl_AddN.py::TestTFLiteAddNLayerTest::test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 5, 'shape': [1, 3]} ]"</error></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 4, 'shape': [2, 8, 5, 2]} ]" time="0.417" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'num_inputs': 4, 'shape': [1, 3]} ]" time="0.353"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e438b0&gt;, params = {'num_inputs': 4, 'shape': [1, 3]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nuz5asx9t'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f34002e3130&gt;&gt;
func_args = [{'Input_0': array([[7., 7., 2.]], dtype=float32), 'Input_1': array([[ 3., -2.,  7.]], dtype=float32), 'Input_2': arra...9., -4.]], dtype=float32), 'Input_3': array([[-10.,   0., -10.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 4, 'shape': [1, 3]} ]" time="0.322" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'num_inputs': 1, 'shape': [2, 8, 5, 2]} ]" time="0.079"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43a60&gt;, params = {'num_inputs': 1, 'shape': [2, 8, 5, 2]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nxs05u3f_'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43a60&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'num_inputs': 5, 'shape': [1, 3]} ]" time="0.257"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e439d0&gt;, params = {'num_inputs': 5, 'shape': [1, 3]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nc_bvgpus'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f34002e3b20&gt;&gt;
func_args = [{'Input_0': array([[7., 7., 2.]], dtype=float32), 'Input_1': array([[ 3., -2.,  7.]], dtype=float32), 'Input_2': arra...4.]], dtype=float32), 'Input_3': array([[-10.,   0., -10.]], dtype=float32), ...}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 4, 'shape': [1, 3]} ]" time="0.439" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 1, 'shape': [2, 8, 5, 2]} ]" time="0.113"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e433a0&gt;, params = {'num_inputs': 1, 'shape': [2, 8, 5, 2]}, ie_device = 'CPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_nbuuzenou'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e433a0&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 5, 'shape': [1, 3]} ]" time="0.518" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 1, 'shape': [2, 8, 5, 2]} ]" time="0.179"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43040&gt;, params = {'num_inputs': 1, 'shape': [2, 8, 5, 2]}, ie_device = 'CPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_na1p8bd56'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43040&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'num_inputs': 1, 'shape': [1, 3]} ]" time="0.128"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43790&gt;, params = {'num_inputs': 1, 'shape': [1, 3]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n1sdejuc1'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43790&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'num_inputs': 5, 'shape': [1, 3]} ]" time="0.386"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43d30&gt;, params = {'num_inputs': 5, 'shape': [1, 3]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n857mjol2'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f3401c17040&gt;&gt;
func_args = [{'Input_0': array([[7., 7., 2.]], dtype=float32), 'Input_1': array([[ 3., -2.,  7.]], dtype=float32), 'Input_2': array([[ 2.,  9., -4.]], dtype=float32), 'Input_3': array([[-10.,   0., -10.]], dtype=float32), ...}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'num_inputs': 1, 'shape': [2, 8, 5, 2]} ]" time="0.150"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43700&gt;, params = {'num_inputs': 1, 'shape': [2, 8, 5, 2]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_neajp_meu'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43700&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'num_inputs': 4, 'shape': [1, 3]} ]" time="0.348"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43c10&gt;, params = {'num_inputs': 4, 'shape': [1, 3]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nzpqur2iz'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f33f061cd00&gt;&gt;
func_args = [{'Input_0': array([[7., 7., 2.]], dtype=float32), 'Input_1': array([[ 3., -2.,  7.]], dtype=float32), 'Input_2': array([[ 2.,  9., -4.]], dtype=float32), 'Input_3': array([[-10.,   0., -10.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'num_inputs': 5, 'shape': [2, 8, 5, 2]} ]" time="0.374"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43940&gt;, params = {'num_inputs': 5, 'shape': [2, 8, 5, 2]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_noelutdon'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f34002e1030&gt;&gt;
func_args = [{'Input_0': array([[[[  7.,   7.],
         [  2.,   3.],
         [ -2.,   7.],
         [  2.,   9.],
         [ -4...-2.,   0.],
         [  0.,  -5.],
         [ -6.,   2.]]]], dtype=float32), ...}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'num_inputs': 4, 'shape': [2, 8, 5, 2]} ]" time="0.366"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43820&gt;, params = {'num_inputs': 4, 'shape': [2, 8, 5, 2]}, ie_device = 'GPU'
precision = 'FP32', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n205qv8hc'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f340031cfd0&gt;&gt;
func_args = [{'Input_0': array([[[[  7.,   7.],
         [  2.,   3.],
         [ -2.,   7.],
         [  2.,   9.],
         [ -4...   [ -2.,   0.],
         [  0.,  -5.],
         [ -6.,   2.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'num_inputs': 4, 'shape': [2, 8, 5, 2]} ]" time="0.414" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'num_inputs': 4, 'shape': [2, 8, 5, 2]} ]" time="0.333"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43b80&gt;, params = {'num_inputs': 4, 'shape': [2, 8, 5, 2]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nirewk8io'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f34002c8310&gt;&gt;
func_args = [{'Input_0': array([[[[  7.,   7.],
         [  2.,   3.],
         [ -2.,   7.],
         [  2.,   9.],
         [ -4...,
         [  8.,   8.],
         [ -2.,   0.],
         [  0.,  -5.],
         [ -6.,   2.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'num_inputs': 1, 'shape': [1, 3]} ]" time="0.127"><failure message="AssertionError: TFLite model is not as you expect it to be:">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43af0&gt;, params = {'num_inputs': 1, 'shape': [1, 3]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n85rew3em'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43af0&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be:

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 5, 'shape': [1, 3]} ]" time="0.503" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'num_inputs': 5, 'shape': [2, 8, 5, 2]} ]" time="0.464" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'num_inputs': 5, 'shape': [2, 8, 5, 2]} ]" time="0.377"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f3401e43ca0&gt;, params = {'num_inputs': 5, 'shape': [2, 8, 5, 2]}, ie_device = 'GPU'
precision = 'FP16', temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nw6d31rzc'

    @pytest.mark.parametrize("params", test_data)
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f34003214b0&gt;&gt;
func_args = [{'Input_0': array([[[[  7.,   7.],
         [  2.,   3.],
         [ -2.,   7.],
         [  2.,   9.],
         [ -4...      [  8.,   8.],
         [ -2.,   0.],
         [  0.,  -5.],
         [ -6.,   2.]]]], dtype=float32), ...}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP16 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.032"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f3401c8f100&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndw5ubwkqm'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f340014b130&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP16 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.018"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f3401c8eec0&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_ndihierd4g'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f34000c3880&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP32 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.013"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f3401c8efe0&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndbudizv4s'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f33f0793b50&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP16 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.019"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f3401c8ef50&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_nde42_x0ao'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f33f077fac0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP32 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.012"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f3401c8eb60&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_nd4ohm2zed'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f33f077c3a0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP32 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.012"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f3401c8f070&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndwda17a8n'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f33f077c8b0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP32 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.018"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f3401c8ee30&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_nd1807agut'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f3401c73400&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP16 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.020"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f3401c8f190&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_nd9eftupw4'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f3401c73130&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0b30&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ba0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c10&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c80&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.012"><failure message="ValueError: Dimensions must be equal, but are 4 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [5,1,4,5,4], [4,3,3,2,2].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e620&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0b30&gt;, &lt;generator object ...r&gt; at 0x7f3401ca0ba0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c10&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c80&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmul369gukz7'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f33f077d120&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 1, 4, 5, 4) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(4, 3, 3, 2, 2) dtype=float32&gt;], control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 4 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [5,1,4,5,4], [4,3,3,2,2].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca07b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0820&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0890&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0900&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.212"><failure message="TypeError: expected a sequence of integers or a single integer, got '&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0890&gt;'">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e350&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca07b0&gt;, &lt;generator object ...r&gt; at 0x7f3401ca0820&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0890&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0900&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulyt4cuell'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:98: in _test
    inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:26: in _prepare_input
    inputs_dict['Input'] = np.float32((1.0 - (-1.0)) * np.random.random_sample(input0_shape) + (-1.0))
numpy/random/mtrand.pyx:437: in numpy.random.mtrand.RandomState.random_sample
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

&gt;   ???
E   TypeError: expected a sequence of integers or a single integer, got '&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0890&gt;'

_common.pyx:307: TypeError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0970&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca09e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0a50&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ac0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.014"><failure message="ValueError: Dimensions must be equal, but are 5 and 4 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=true, adj_y=false](Input, Input1)' with input shapes: [5,1,5,4], [4,4,3].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e740&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0970&gt;, &lt;generator object &lt;...r&gt; at 0x7f3401ca09e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0a50&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ac0&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulb4diiqyu'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f33f077c280&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: true
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 1, 5, 4) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(4, 4, 3) dtype=float32&gt;], control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 5 and 4 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=true, adj_y=false](Input, Input1)' with input shapes: [5,1,5,4], [4,4,3].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca07b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0820&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0890&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0900&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.014"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e6b0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca07b0&gt;, &lt;generator object ...r&gt; at 0x7f3401ca0820&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0890&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0900&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmul4tr11w23'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f34000c13f0&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;], control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0970&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca09e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0a50&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ac0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.014"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e590&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0970&gt;, &lt;generator object &lt;...r&gt; at 0x7f3401ca09e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0a50&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ac0&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulka_tto3e'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e590&gt;, args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0970&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca09e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0a50&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ac0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.015"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e230&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0970&gt;, &lt;generator object &lt;...r&gt; at 0x7f3401ca09e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0a50&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ac0&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulxwgex4aa'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e230&gt;, args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0b30&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ba0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c10&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c80&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.012"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e2c0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0b30&gt;, &lt;generator object ...r&gt; at 0x7f3401ca0ba0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c10&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c80&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulg3rqilhy'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f34000c1360&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;], control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0970&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca09e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0a50&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ac0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.026"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e3e0&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0970&gt;, &lt;generator object &lt;...r&gt; at 0x7f3401ca09e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0a50&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ac0&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulbsxyx7fb'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e3e0&gt;, args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca07b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0820&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0890&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0900&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.020"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e500&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca07b0&gt;, &lt;generator object ...r&gt; at 0x7f3401ca0820&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0890&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0900&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmul6ku8uc3v'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f340014be20&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;], control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0b30&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0ba0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c10&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c80&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.024"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f3401c8e7d0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f3401ca0b30&gt;, &lt;generator object ...r&gt; at 0x7f3401ca0ba0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c10&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f3401ca0c80&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmuljqelj8pt'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f340014a680&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;], control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase time="0.004" /><testcase classname="pytest" name="internal" time="0.000"><error message="internal error">Traceback (most recent call last):
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/_pytest/main.py", line 271, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/_pytest/main.py", line 325, in _main
    config.hook.pytest_runtestloop(session=session)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_hooks.py", line 493, in __call__
    return self._hookexec(self.name, self._hookimpls, kwargs, firstresult)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_manager.py", line 115, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_callers.py", line 152, in _multicall
    return outcome.get_result()
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_result.py", line 114, in get_result
    raise exc.with_traceback(exc.__traceback__)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/pluggy/_callers.py", line 77, in _multicall
    res = hook_impl.function(*args)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/xdist/dsession.py", line 123, in pytest_runtestloop
    self.loop_once()
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/xdist/dsession.py", line 148, in loop_once
    call(**kwargs)
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/xdist/dsession.py", line 273, in worker_collectionfinish
    self.sched.schedule()
  File "/home/chaitanyasai/.local/lib/python3.10/site-packages/xdist/scheduler/each.py", line 134, in schedule
    pending[:] = range(len(self.node2collection[node]))
KeyError: &lt;WorkerController gw2&gt;</error></testcase></testsuite></testsuites>
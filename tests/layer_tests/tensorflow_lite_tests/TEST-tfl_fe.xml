<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="157" skipped="0" tests="180" time="54.388" timestamp="2024-01-25T15:18:48.504468" hostname="chaitanyasai-HP-ProBook-440-G5"><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP16 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.026"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7fbd44a85ff0&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_ndx3i531gi'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd93c79990&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP16 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.029"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f41edb7e020&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_nd9i57ngjt'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f423cc85990&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP16 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.030"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7ff531cfa080&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_ndena69b2u'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff581281990&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP32 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.012"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7fbd44a861a0&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndnm7u49k4'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd44a69870&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP32 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.012"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f41edb7e1d0&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_nde85gs910'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41edb61870&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP32 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.014"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7ff531cfa230&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_nd2cq9a5q9'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff531cdd870&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP32 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.012"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7fbd44a85f60&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_nd0xxmuab_'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd4498b9a0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP32 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.014"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f41edb7df90&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_ndjmv6bm2y'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41eda7f9a0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP32 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.016"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7ff531cf9ff0&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_ndzkzpsbdr'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff5303f79a0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP32 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.016"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7fbd44a85de0&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_ndyy5n5uve'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c748700&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP32 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.018"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f41edb7de10&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_ndo8m_g134'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec0cc700&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP32 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.014"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7ff531cf9e70&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_ndbs_bbt8f'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff530448700&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP16 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.015"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7fbd44a86080&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_nd_9i68xne'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c749120&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP16 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.013"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f41edb7e0b0&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_nd3ovgleql'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec0cd120&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:CPU - precision:FP16 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.018"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7ff531cfa110&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_to_space_ndjdprfrq2'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff530449120&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP16 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.015"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7fbd44a862c0&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndo97a7emw'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c749630&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP16 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.015"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f41edb7e2f0&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndql7z6q_0'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec0cd630&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP16 - params:{'shape': [20, 9, 18, 23], 'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]]} ]" time="0.017"><failure message="ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7ff531cfa350&gt;
params = {'block_shape': [13, 10], 'crops': [[2, 3], [1, 1]], 'shape': [20, 9, 18, 23]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndubmlq8lf'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff530449630&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(20, 9, 18, 23) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 13 but is 20 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [20,9,18,23], [2], [2,2] and with computed input tensors: input[1] = &lt;13 10&gt;, input[2] = &lt;[2 3][1 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP32 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.044"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7fbd44a86110&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndvjvngeyw'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd44a697e0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP32 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.031"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f41edb7e140&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndxz8erf7h'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41edb617e0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP32 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.015"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7ff531cfa1a0&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndh5azz9sv'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff531cdd7e0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP16 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.016"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7fbd44a86230&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndupg37qgn'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd44a69ab0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP16 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.016"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7f41edb7e260&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndimfjohmt'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41edb61ab0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest" name="test_batch_to_space_nd[ ie_device:GPU - precision:FP16 - params:{'shape': [5, 8, 6, 10], 'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]]} ]" time="0.015"><failure message="ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.">self = &lt;test_tfl_BatchToSpaceND.TestTFLiteBatchToSpaceNDLayerTest object at 0x7ff531cfa2c0&gt;
params = {'block_shape': [2, 5], 'crops': [[2, 3], [2, 1]], 'shape': [5, 8, 6, 10]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_to_space_ndldy468dc'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_to_space_nd(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchToSpaceND.py:24: in make_model
    tf.batch_to_space(place_holder, params['block_shape'], params['crops'],
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff531cddab0&gt;
node_def = name: "BatchToSpaceND"
op: "BatchToSpaceND"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tcrops"
  value {
    type: DT_INT32
  }
}
attr {
  key: "Tblock_shape"
  value {
    type: DT_INT32
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(5, 8, 6, 10) dtype=float32&gt;, &lt;tf.Tensor 'BatchToSpaceND/block_shape:0' shape=(2,) dtype=int32&gt;, &lt;tf.Tensor 'BatchToSpaceND/crops:0' shape=(2, 2) dtype=int32&gt;]
control_inputs = []
op_def = name: "BatchToSpaceND"
input_arg {
  name: "input"
  type_attr: "T"
}
input_arg {
  name: "block_shape"
  type_attr: "...ult_value {
    type: DT_INT32
  }
  allowed_values {
    list {
      type: DT_INT32
      type: DT_INT64
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimension size must be evenly divisible by 2 but is 5 for '{{node BatchToSpaceND}} = BatchToSpaceND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tcrops=DT_INT32](Input, BatchToSpaceND/block_shape, BatchToSpaceND/crops)' with input shapes: [5,8,6,10], [2], [2,2] and with computed input tensors: input[1] = &lt;2 5&gt;, input[2] = &lt;[2 3][2 1]&gt;.

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c2e0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.017"><failure message="ValueError: Dimensions must be equal, but are 3 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=true, adj_y=false](Input, Input1)' with input shapes: [2,3,3,4], [2,3,2].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf9810&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c190&gt;, &lt;generator object &lt;...r&gt; at 0x7ff531d0c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c2e0&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulrphjksvd'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff530449f30&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: true
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(2, 3, 3, 4) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(2, 3, 2) dtype=float32&gt;]
control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 3 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=true, adj_y=false](Input, Input1)' with input shapes: [2,3,3,4], [2,3,2].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44a1bf40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448980b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898120&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.022"><failure message="ValueError: Dimensions must be equal, but are 4 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4], [3,2,3].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a85540&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44a1bf40&gt;, &lt;generator object ...r&gt; at 0x7fbd44898040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448980b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898120&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulfj76i9cm'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c749f30&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(2, 3, 4) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(3, 2, 3) dtype=float32&gt;]
control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 4 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4], [3,2,3].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c120&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c270&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.022"><failure message="ValueError: Dimensions must be equal, but are 3 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=true, adj_y=false](Input, Input1)' with input shapes: [2,3,3,4], [2,3,2].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d450&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c120&gt;, &lt;generator object &lt;...r&gt; at 0x7f41ed98c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c270&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulgsd3ukpr'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec0cdf30&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: true
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(2, 3, 3, 4) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(2, 3, 2) dtype=float32&gt;]
control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 3 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=true, adj_y=false](Input, Input1)' with input shapes: [2,3,3,4], [2,3,2].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c2e0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.010"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf9660&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c190&gt;, &lt;generator object &lt;...r&gt; at 0x7ff531d0c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c2e0&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulhd9abnc7'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf9660&gt;
args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531e8bf40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c0b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c120&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.015"><failure message="ValueError: Dimensions must be equal, but are 4 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4], [3,2,3].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf9120&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531e8bf40&gt;, &lt;generator object ...r&gt; at 0x7ff531d0c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c0b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c120&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulcfk85dbr'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff53044a440&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(2, 3, 4) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(3, 2, 3) dtype=float32&gt;]
control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 4 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4], [3,2,3].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c2e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c430&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.014"><failure message="ValueError: Dimensions must be equal, but are 2 and 1 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4,3,2], [3,2,3,1,1].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d9f0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c2e0&gt;, &lt;generator object ...r&gt; at 0x7f41ed98c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c430&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulotvs9nw6'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec0cf640&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(2, 3, 4, 3, 2) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(3, 2, 3, 1, 1) dtype=float32&gt;]
control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 2 and 1 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4,3,2], [3,2,3,1,1].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448982e0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.014"><failure message="ValueError: Dimensions must be equal, but are 3 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=true, adj_y=false](Input, Input1)' with input shapes: [2,3,3,4], [2,3,2].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a85930&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898190&gt;, &lt;generator object &lt;...r&gt; at 0x7fbd44898200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448982e0&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmuldbirjy3h'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c74b640&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: true
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(2, 3, 3, 4) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(2, 3, 2) dtype=float32&gt;]
control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 3 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=true, adj_y=false](Input, Input1)' with input shapes: [2,3,3,4], [2,3,2].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c4a0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.013"><failure message="ValueError: Dimensions must be equal, but are 2 and 1 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4,3,2], [3,2,3,1,1].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf98a0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c350&gt;, &lt;generator object ...r&gt; at 0x7ff531d0c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c4a0&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmul_q8bw89d'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff53044b910&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(2, 3, 4, 3, 2) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(3, 2, 3, 1, 1) dtype=float32&gt;]
control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 2 and 1 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4,3,2], [3,2,3,1,1].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448983c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448984a0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.023"><failure message="ValueError: Dimensions must be equal, but are 2 and 1 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4,3,2], [3,2,3,1,1].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a85810&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898350&gt;, &lt;generator object ...r&gt; at 0x7fbd448983c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448984a0&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmul7zf_0lw7'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c74b7f0&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(2, 3, 4, 3, 2) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(3, 2, 3, 1, 1) dtype=float32&gt;]
control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 2 and 1 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4,3,2], [3,2,3,1,1].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c2e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c430&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.013"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d690&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c2e0&gt;, &lt;generator object ...r&gt; at 0x7f41ed98c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c430&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulc2t12hak'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec0cf7f0&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c4a0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.013"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf96f0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c350&gt;, &lt;generator object ...r&gt; at 0x7ff531d0c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c4a0&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmul6191i2uq'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff530448f70&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448982e0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.008"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a85420&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898190&gt;, &lt;generator object &lt;...r&gt; at 0x7fbd44898200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448982e0&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulkjhpstm1'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a85420&gt;
args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41edb13ed0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41edb13f40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c0b0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.014"><failure message="ValueError: Dimensions must be equal, but are 4 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4], [3,2,3].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d0c0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41edb13ed0&gt;, &lt;generator object ...r&gt; at 0x7f41edb13f40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c0b0&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmuly7p9k548'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec0cfd90&gt;
node_def = name: "BatchMatmul"
op: "BatchMatMulV2"
attr {
  key: "adj_y"
  value {
    b: false
  }
}
attr {
  key: "adj_x"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=(2, 3, 4) dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=(3, 2, 3) dtype=float32&gt;]
control_inputs = []
op_def = name: "BatchMatMulV2"
input_arg {
  name: "x"
  type_attr: "T"
}
input_arg {
  name: "y"
  type_attr: "T"
}
output_arg..."bool"
  default_value {
    b: false
  }
}
attr {
  name: "adj_y"
  type: "bool"
  default_value {
    b: false
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Dimensions must be equal, but are 4 and 2 for '{{node BatchMatmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Input, Input1)' with input shapes: [2,3,4], [3,2,3].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44a1bf40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448980b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898120&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.014"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a85090&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44a1bf40&gt;, &lt;generator object ...r&gt; at 0x7fbd44898040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448980b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898120&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulqly_6jm9'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c74be20&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c4a0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.018"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf9a50&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c350&gt;, &lt;generator object ...r&gt; at 0x7ff531d0c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c4a0&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmuldab46k7l'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff5303241f0&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c2e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c430&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.014"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d840&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c2e0&gt;, &lt;generator object ...r&gt; at 0x7f41ed98c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c430&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulrrbxlifm'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec11fe20&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448983c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448984a0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.015"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a85660&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898350&gt;, &lt;generator object ...r&gt; at 0x7fbd448983c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448984a0&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulrpicvtqj'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c7f4280&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531e8bf40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c0b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c120&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.013"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf95d0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531e8bf40&gt;, &lt;generator object ...r&gt; at 0x7ff531d0c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c0b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c120&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulw32vucru'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff530324d30&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c2e0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c430&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.014"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d4e0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c2e0&gt;, &lt;generator object ...r&gt; at 0x7f41ed98c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c430&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulkeronvos'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec11cc10&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44a1bf40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448980b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898120&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.015"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a856f0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44a1bf40&gt;, &lt;generator object ...r&gt; at 0x7fbd44898040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448980b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898120&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmul_j32oeft'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c7f4ca0&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c120&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c270&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.009"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d960&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c120&gt;, &lt;generator object &lt;...r&gt; at 0x7f41ed98c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c270&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulg0binab5'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d960&gt;
args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c2e0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.010"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf99c0&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c190&gt;, &lt;generator object &lt;...r&gt; at 0x7ff531d0c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c2e0&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmuls7lvx9e6'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf99c0&gt;
args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c120&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c270&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.010"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d7b0&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c120&gt;, &lt;generator object &lt;...r&gt; at 0x7f41ed98c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c270&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmuly50cp5cd'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d7b0&gt;
args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c2e0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.010"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf94b0&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c190&gt;, &lt;generator object &lt;...r&gt; at 0x7ff531d0c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c2e0&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulrwqw_oe3'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf94b0&gt;
args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448982e0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.012"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a85780&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898190&gt;, &lt;generator object &lt;...r&gt; at 0x7fbd44898200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448982e0&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulbb6fx6ik'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a85780&gt;
args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41edb13ed0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41edb13f40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c0b0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.017"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d8d0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41edb13ed0&gt;, &lt;generator object ...r&gt; at 0x7f41edb13f40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c0b0&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmuli2ylnegq'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec11d750&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531e8bf40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c0b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c120&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.016"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf9780&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531e8bf40&gt;, &lt;generator object ...r&gt; at 0x7ff531d0c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c0b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c120&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmule5pzb9al'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff530325870&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448982e0&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.009"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a855d0&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898190&gt;, &lt;generator object &lt;...r&gt; at 0x7fbd44898200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898270&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448982e0&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmul91anyjaz'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a855d0&gt;
args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448983c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448984a0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.019"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a854b0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898350&gt;, &lt;generator object ...r&gt; at 0x7fbd448983c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448984a0&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulqbvdilcs'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c7f57e0&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41edb13ed0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41edb13f40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c0b0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.012"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d570&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41edb13ed0&gt;, &lt;generator object ...r&gt; at 0x7f41edb13f40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c0b0&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmullvc34t7e'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec11c790&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c4a0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.015"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf9540&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531d0c350&gt;, &lt;generator object ...r&gt; at 0x7ff531d0c3c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c4a0&gt;)}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmuly7nv91fj'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff530325cf0&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898350&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448983c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448984a0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.012"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a859c0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44898350&gt;, &lt;generator object ...r&gt; at 0x7fbd448983c0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898430&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448984a0&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmul8mtthl40'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c7f5c60&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:CPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c120&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c270&gt;), 'adjoint_a': True, 'adjoint_b': False} ]" time="0.009"><failure message="IndexError: tuple index out of range">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d600&gt;
params = {'adjoint_a': True, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41ed98c120&gt;, &lt;generator object &lt;...r&gt; at 0x7f41ed98c190&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c200&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c270&gt;)}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_batch_matmulcs3qyqqg'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:50: in make_model
    placeholder0_shape = self._swap_last_two_dims(*placeholder0_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d600&gt;
args = ()

    def _swap_last_two_dims(self, *args):
        """Return a tuple with the last two dimensions swapped."""
&gt;       return args[:-2] + (args[-1],) + (args[-2],)
E       IndexError: tuple index out of range

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:35: IndexError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP32 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41edb13ed0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41edb13f40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c0b0&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.013"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7f41edb7d720&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7f41edb13ed0&gt;, &lt;generator object ...r&gt; at 0x7f41edb13f40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7f41ed98c0b0&gt;)}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulq4fw3_f6'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7f41ec11e170&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44a1bf40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448980b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898120&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.012"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7fbd44a858a0&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7fbd44a1bf40&gt;, &lt;generator object ...r&gt; at 0x7fbd44898040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd448980b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7fbd44898120&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulbi4urev6'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7fbd3c7f48b0&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest" name="test_batch_matmul[ ie_device:GPU - precision:FP16 - params:{'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531e8bf40&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c0b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c120&gt;), 'adjoint_a': False, 'adjoint_b': False} ]" time="0.016"><failure message="ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].">self = &lt;test_tfl_BatchMatmul.TestTFLiteBatchMatmulLayerTest object at 0x7ff531cf9930&gt;
params = {'adjoint_a': False, 'adjoint_b': False, 'shapes': (&lt;generator object &lt;genexpr&gt; at 0x7ff531e8bf40&gt;, &lt;generator object ...r&gt; at 0x7ff531d0c040&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c0b0&gt;, &lt;generator object &lt;genexpr&gt; at 0x7ff531d0c120&gt;)}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_batch_matmulgnc2_0w1'

    @pytest.mark.parametrize("params", test_params)
    @pytest.mark.nightly
    def test_batch_matmul(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_BatchMatmul.py:58: in make_model
    tf.matmul(input0_tensor, input1_tensor, adjoint_a=adj_a, adjoint_b=adj_b, name=self.outputs[0])
../../../.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142: in wrapper
    return op(*args, **kwargs)
../../../.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153: in error_handler
    raise e.with_traceback(filtered_tb) from None
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

graph = &lt;tensorflow.python.framework.ops.Graph object at 0x7ff530324940&gt;
node_def = name: "BatchMatmul"
op: "MatMul"
attr {
  key: "transpose_b"
  value {
    b: false
  }
}
attr {
  key: "transpose_a"
  value {
    b: false
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

inputs = [&lt;tf.Tensor 'Input:0' shape=() dtype=float32&gt;, &lt;tf.Tensor 'Input1:0' shape=() dtype=float32&gt;]
control_inputs = []
op_def = name: "MatMul"
input_arg {
  name: "a"
  type_attr: "T"
}
input_arg {
  name: "b"
  type_attr: "T"
}
output_arg {
  na...: DT_UINT16
      type: DT_UINT32
      type: DT_UINT64
      type: DT_COMPLEX64
      type: DT_COMPLEX128
    }
  }
}

extract_traceback = True

    @tf_export("__internal__.create_c_op", v1=[])
    @traceback_utils.filter_traceback
    def _create_c_op(graph,
                     node_def,
                     inputs,
                     control_inputs,
                     op_def=None,
                     extract_traceback=True):
      """Creates a TF_Operation.
    
      Args:
        graph: a `Graph`.
        node_def: `node_def_pb2.NodeDef` for the operation to create.
        inputs: A flattened list of `Tensor`s. This function handles grouping
          tensors into lists as per attributes in the `node_def`.
        control_inputs: A list of `Operation`s to set as control dependencies.
        op_def: Optional. `op_def_pb2.OpDef` for the operation to create. If not
          specified, is looked up from the `graph` using `node_def.op`.
        extract_traceback: if True, extract the current Python traceback to the
          TF_Operation.
    
      Returns:
        A wrapped TF_Operation*.
      """
      if op_def is None:
        op_def = graph.op_def_for_type(node_def.op)  # pylint: disable=protected-access
      # TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.
      # Refactor so we don't have to do this here.
      inputs = _reconstruct_sequence_inputs(op_def, inputs, node_def.attr)
      # pylint: disable=protected-access
      with graph._c_graph.get() as c_graph:
        op_desc = pywrap_tf_session.TF_NewOperation(c_graph,
                                                    compat.as_str(node_def.op),
                                                    compat.as_str(node_def.name))
      if node_def.device:
        pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))
      # Add inputs
      for op_input in inputs:
        if isinstance(op_input, (list, tuple)):
          pywrap_tf_session.TF_AddInputList(op_desc,
                                            [t._as_tf_output() for t in op_input])
        else:
          pywrap_tf_session.TF_AddInput(op_desc, op_input._as_tf_output())
    
      # Add control inputs
      for control_input in control_inputs:
        pywrap_tf_session.TF_AddControlInput(op_desc, control_input._c_op)
      # pylint: enable=protected-access
    
      # Add attrs
      for name, attr_value in node_def.attr.items():
        serialized = attr_value.SerializeToString()
        # TODO(skyewm): this creates and deletes a new TF_Status for every attr.
        # It might be worth creating a convenient way to re-use the same status.
        pywrap_tf_session.TF_SetAttrValueProto(op_desc, compat.as_str(name),
                                               serialized)
    
      try:
        c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
      except errors.InvalidArgumentError as e:
        # Convert to ValueError for backwards compatibility.
&gt;       raise ValueError(e.message)
E       ValueError: Shape must be rank 2 but is rank 0 for '{{node BatchMatmul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Input, Input1)' with input shapes: [], [].

../../../.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1020: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="2.793"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7ca30&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value9vb57jjl'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e43d0dc0&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="2.298"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84580&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value28jiwprn'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c7cf130&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="3.767"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([2, 5, 3, 7, 7, 6, 4, 1])}; framework_res={'ArgValue': array([7, 3, 2, 4, 6, 2, 7, 0])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbbcd0&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuez9hftwvc'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbbcd0&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuez9hftwvc'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuez9hftwvc', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([2, 5, 3, 7, 7, 6, 4, 1])}; framework_res={'ArgValue': array([7, 3, 2, 4, 6, 2, 7, 0])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [9, 8], 'axis': None} ]" time="1.469" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="1.057" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.198"><failure message="struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3fc70&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuejh9mmkkx'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
tests/layer_tests/common/tflite_layer_test_class.py:54: in check_tflite_model_has_only_allowed_ops
    model = utils.read_model(self.model_path)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:61: in read_model
    model = convert_bytearray_to_object(model_bytearray)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:40: in convert_bytearray_to_object
    model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17784: in GetRootAsModel
    return cls.GetRootAs(buf, offset)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17776: in GetRootAs
    n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

packer_type = &lt;_struct.Struct object at 0x7f41f08c9120&gt;, buf = bytearray(b'')
head = 0

    def Get(packer_type, buf, head):
        """ Get decodes a value at buf[head] using `packer_type`. """
&gt;       return packer_type.unpack_from(memoryview_type(buf), head)[0]
E       struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)

../../../.local/lib/python3.10/site-packages/flatbuffers/encode.py:26: error</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.375"><failure message="AssertionError: TFLite model is not as you expect it to be: ARG_MIN">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84730&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuee4ynrb1p'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84730&gt;

    def check_tflite_model_has_only_allowed_ops(self):
        if self.allowed_ops is None:
            return
        BO = utils.schema_fb.BuiltinOperator
        builtin_operators = {getattr(BO, name): name for name in dir(BO) if not name.startswith("_")}
        model = utils.read_model(self.model_path)
    
        op_names = []
        for op in model.operatorCodes:
            assert op.customCode is None, "Encountered custom operation in the model"
            deprecated_code = op.deprecatedBuiltinCode
            deprecated_vs_normal = utils.schema_fb.BuiltinOperator.PLACEHOLDER_FOR_GREATER_OP_CODES
            if deprecated_code &lt; deprecated_vs_normal:
                op_names.append(builtin_operators[op.deprecatedBuiltinCode])
            else:
                op_names.append(builtin_operators[op.builtinCode])
        op_names = sorted(op_names)
        if isinstance(self.allowed_ops, tuple):
            passed = False
            for allowed_ops_var in self.allowed_ops:
                if op_names == allowed_ops_var:
                    passed = True
                    break
            assert passed, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
        else:
&gt;           assert op_names == self.allowed_ops, "TFLite model is not as you expect it to be: " + ", ".join(op_names)
E           AssertionError: TFLite model is not as you expect it to be: ARG_MIN

tests/layer_tests/common/tflite_layer_test_class.py:74: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.511"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf86a0&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuevhnu1xe3'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff530406b30&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.510"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a846a0&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value59e4pvhe'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd449638b0&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.494"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8970&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueyc4v2e15'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff5303702e0&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5....3., -7.]],

        [[ 1.,  7., -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.448"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2, 3, 2]],&#10;&#10;       [[1, 1, 3]],&#10;&#10;       [[3, 1, 0]],&#10;&#10;       [[2, 2, 0]]])}; framework_res={'ArgValue': array([[[1, 2, 0]],&#10;&#10;       [[2, 2, 2]],&#10;&#10;       [[1, 0, 1]],&#10;&#10;       [[1, 3, 2]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84100&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuehrx8ub2h'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84100&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuehrx8ub2h'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuehrx8ub2h', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2, 3, 2]],
E       
E              [[1, 1, 3]],
E       
E              [[3, 1, 0]],
E       
E              [[2, 2, 0]]])}; framework_res={'ArgValue': array([[[1, 2, 0]],
E       
E              [[2, 2, 2]],
E       
E              [[1, 0, 1]],
E       
E              [[1, 3, 2]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.549" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.388"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84220&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuexbike3ln'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c7cd660&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...-6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.498"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[0],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[0],&#10;        [2],&#10;        [0],&#10;        [0]],&#10;&#10;       [[1],&#10;        [1],&#10;        [0],&#10;        [2]],&#10;&#10;       [[1],&#10;        [1],&#10;        [1],&#10;        [1]]])}; framework_res={'ArgValue': array([[[2],&#10;        [2],&#10;        [2],&#10;        [2]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [2],&#10;        [0],&#10;        [2]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8100&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuebybefhp2'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8100&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuebybefhp2'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuebybefhp2', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[0],
E               [0],
E               [1],
E               [1]],
E       
E              [[0],
E               [2],
E               [0],
E               [0]],
E       
E              [[1],
E               [1],
E               [0],
E               [2]],
E       
E              [[1],
E               [1],
E               [1],
E               [1]]])}; framework_res={'ArgValue': array([[[2],
E               [2],
E               [2],
E               [2]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [2],
E               [0],
E               [2]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.447"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a843d0&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value_ujk85aa'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd44962530&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...-6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.505"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2],&#10;        [2],&#10;        [2],&#10;        [2]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [2],&#10;        [0],&#10;        [2]]])}; framework_res={'ArgValue': array([[[1, 2, 0]],&#10;&#10;       [[2, 2, 2]],&#10;&#10;       [[1, 0, 1]],&#10;&#10;       [[1, 3, 2]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8220&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value2ipye9b0'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8220&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value2ipye9b0'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value2ipye9b0', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2],
E               [2],
E               [2],
E               [2]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [2],
E               [0],
E               [2]]])}; framework_res={'ArgValue': array([[[1, 2, 0]],
E       
E              [[2, 2, 2]],
E       
E              [[1, 0, 1]],
E       
E              [[1, 3, 2]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.539" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.408"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8850&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value_xhkosir'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff528716f80&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.514" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.445"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8460&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value6_wmsxlv'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff528715450&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.534" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.452"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8b20&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value21toc88v'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff5303cdab0&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.520"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([7, 4, 2, 4, 3, 0, 4, 2, 2])}; framework_res={'ArgValue': array([5, 7, 0, 2, 2, 1, 5, 3, 0])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47df0&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuetp2t0d7x'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47df0&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuetp2t0d7x'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuetp2t0d7x', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([7, 4, 2, 4, 3, 0, 4, 2, 2])}; framework_res={'ArgValue': array([5, 7, 0, 2, 2, 1, 5, 3, 0])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.428"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c640&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuelz8o8y0g'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e474ac80&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.526"><failure message="ValueError: operands could not be broadcast together with shapes (8,) (9,)">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbbf10&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value8r5njvb7'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
tests/layer_tests/common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([2, 5, 3, 7, 7, 6, 4, 1])
ref_array = array([7, 4, 2, 4, 3, 0, 4, 2, 2]), atol = 0.05, rtol = 0.05

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
            abs_diff = np.absolute(cur_array ^ ref_array)
        else:
&gt;           abs_diff = np.absolute(cur_array - ref_array)
E           ValueError: operands could not be broadcast together with shapes (8,) (9,)

tests/layer_tests/common/utils/common_utils.py:102: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.547"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[0],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[0],&#10;        [2],&#10;        [0],&#10;        [0]],&#10;&#10;       [[1],&#10;        [1],&#10;        [0],&#10;        [2]],&#10;&#10;       [[1],&#10;        [1],&#10;        [1],&#10;        [1]]])}; framework_res={'ArgValue': array([[[1, 2, 0]],&#10;&#10;       [[2, 2, 2]],&#10;&#10;       [[1, 0, 1]],&#10;&#10;       [[1, 3, 2]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47bb0&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuekg12h6on'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47bb0&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuekg12h6on'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuekg12h6on', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[0],
E               [0],
E               [1],
E               [1]],
E       
E              [[0],
E               [2],
E               [0],
E               [0]],
E       
E              [[1],
E               [1],
E               [0],
E               [2]],
E       
E              [[1],
E               [1],
E               [1],
E               [1]]])}; framework_res={'ArgValue': array([[[1, 2, 0]],
E       
E              [[2, 2, 2]],
E       
E              [[1, 0, 1]],
E       
E              [[1, 3, 2]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.534"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[1, 2, 0]],&#10;&#10;       [[2, 2, 2]],&#10;&#10;       [[1, 0, 1]],&#10;&#10;       [[1, 3, 2]]])}; framework_res={'ArgValue': array([[[0],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[0],&#10;        [2],&#10;        [0],&#10;        [0]],&#10;&#10;       [[1],&#10;        [1],&#10;        [0],&#10;        [2]],&#10;&#10;       [[1],&#10;        [1],&#10;        [1],&#10;        [1]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3fb50&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuep5jis_l1'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3fb50&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuep5jis_l1'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuep5jis_l1', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[1, 2, 0]],
E       
E              [[2, 2, 2]],
E       
E              [[1, 0, 1]],
E       
E              [[1, 3, 2]]])}; framework_res={'ArgValue': array([[[0],
E               [0],
E               [1],
E               [1]],
E       
E              [[0],
E               [2],
E               [0],
E               [0]],
E       
E              [[1],
E               [1],
E               [0],
E               [2]],
E       
E              [[1],
E               [1],
E               [1],
E               [1]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.377"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8340&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuer_fk00oh'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff52871e7d0&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...-6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.502"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[1, 2, 0]],&#10;&#10;       [[2, 2, 2]],&#10;&#10;       [[1, 0, 1]],&#10;&#10;       [[1, 3, 2]]])}; framework_res={'ArgValue': array([[[2, 3, 2]],&#10;&#10;       [[1, 1, 3]],&#10;&#10;       [[3, 1, 0]],&#10;&#10;       [[2, 2, 0]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47b20&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuenr_cg_yb'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47b20&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuenr_cg_yb'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuenr_cg_yb', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[1, 2, 0]],
E       
E              [[2, 2, 2]],
E       
E              [[1, 0, 1]],
E       
E              [[1, 3, 2]]])}; framework_res={'ArgValue': array([[[2, 3, 2]],
E       
E              [[1, 1, 3]],
E       
E              [[3, 1, 0]],
E       
E              [[2, 2, 0]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.546"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2, 3, 2]],&#10;&#10;       [[1, 1, 3]],&#10;&#10;       [[3, 1, 0]],&#10;&#10;       [[2, 2, 0]]])}; framework_res={'ArgValue': array([[[0],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[0],&#10;        [2],&#10;        [0],&#10;        [0]],&#10;&#10;       [[1],&#10;        [1],&#10;        [0],&#10;        [2]],&#10;&#10;       [[1],&#10;        [1],&#10;        [1],&#10;        [1]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c130&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value1d0opjnn'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c130&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value1d0opjnn'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value1d0opjnn', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2, 3, 2]],
E       
E              [[1, 1, 3]],
E       
E              [[3, 1, 0]],
E       
E              [[2, 2, 0]]])}; framework_res={'ArgValue': array([[[0],
E               [0],
E               [1],
E               [1]],
E       
E              [[0],
E               [2],
E               [0],
E               [0]],
E       
E              [[1],
E               [1],
E               [0],
E               [2]],
E       
E              [[1],
E               [1],
E               [1],
E               [1]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.428"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8580&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueo0zv9pxi'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff5303cfeb0&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.500" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.415"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c370&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueik69lrme'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e43d1930&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.444"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8a00&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuejf3x092s'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff5304a2050&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5....3., -7.]],

        [[ 1.,  7., -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.525"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[1, 2, 0]],&#10;&#10;       [[2, 2, 2]],&#10;&#10;       [[1, 0, 1]],&#10;&#10;       [[1, 3, 2]]])}; framework_res={'ArgValue': array([[[2],&#10;        [2],&#10;        [2],&#10;        [2]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [2],&#10;        [0],&#10;        [2]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47fa0&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueojr4pfyr'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47fa0&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueojr4pfyr'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueojr4pfyr', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[1, 2, 0]],
E       
E              [[2, 2, 2]],
E       
E              [[1, 0, 1]],
E       
E              [[1, 3, 2]]])}; framework_res={'ArgValue': array([[[2],
E               [2],
E               [2],
E               [2]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [2],
E               [0],
E               [2]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.476" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.523"><failure message="ValueError: operands could not be broadcast together with shapes (9,) (8,)">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbbb20&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuemog_8kb5'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
tests/layer_tests/common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([5, 7, 0, 2, 2, 1, 5, 3, 0])
ref_array = array([7, 3, 2, 4, 6, 2, 7, 0]), atol = 0.0001, rtol = 0.0001

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
            abs_diff = np.absolute(cur_array ^ ref_array)
        else:
&gt;           abs_diff = np.absolute(cur_array - ref_array)
E           ValueError: operands could not be broadcast together with shapes (9,) (8,)

tests/layer_tests/common/utils/common_utils.py:102: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.400"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84460&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value8q6uamba'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c725cc0&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.420"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c520&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuek204ou82'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e43d1150&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.504" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.504"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([5, 7, 0, 2, 2, 1, 5, 3, 0])}; framework_res={'ArgValue': array([7, 4, 2, 4, 3, 0, 4, 2, 2])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47a90&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value_9f4ci92'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a47a90&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value_9f4ci92'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value_9f4ci92', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([5, 7, 0, 2, 2, 1, 5, 3, 0])}; framework_res={'ArgValue': array([7, 4, 2, 4, 3, 0, 4, 2, 2])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.558"><failure message="ValueError: operands could not be broadcast together with shapes (9,) (8,)">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3fe20&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuepyvvq6gl'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
tests/layer_tests/common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([7, 4, 2, 4, 3, 0, 4, 2, 2])
ref_array = array([7, 3, 2, 4, 6, 2, 7, 0]), atol = 0.05, rtol = 0.05

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
            abs_diff = np.absolute(cur_array ^ ref_array)
        else:
&gt;           abs_diff = np.absolute(cur_array - ref_array)
E           ValueError: operands could not be broadcast together with shapes (9,) (8,)

tests/layer_tests/common/utils/common_utils.py:102: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.540"><failure message="ValueError: operands could not be broadcast together with shapes (8,) (9,)">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbbdf0&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuep9jp_c54'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
tests/layer_tests/common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([7, 3, 2, 4, 6, 2, 7, 0])
ref_array = array([7, 4, 2, 4, 3, 0, 4, 2, 2]), atol = 0.05, rtol = 0.05

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
            abs_diff = np.absolute(cur_array ^ ref_array)
        else:
&gt;           abs_diff = np.absolute(cur_array - ref_array)
E           ValueError: operands could not be broadcast together with shapes (8,) (9,)

tests/layer_tests/common/utils/common_utils.py:102: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.426"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a844f0&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuea19r_ui3'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c516d70&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.517" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.517"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([2, 5, 3, 7, 7, 6, 4, 1])}; framework_res={'ArgValue': array([7, 3, 2, 4, 6, 2, 7, 0])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbba90&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value46m05h4d'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbba90&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value46m05h4d'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value46m05h4d', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([2, 5, 3, 7, 7, 6, 4, 1])}; framework_res={'ArgValue': array([7, 3, 2, 4, 6, 2, 7, 0])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.447"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a848e0&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuet8w7hho2'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c7cc130&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5....3., -7.]],

        [[ 1.,  7., -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.534"><failure message="ValueError: operands could not be broadcast together with shapes (8,) (9,)">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3fa30&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueryqljel0'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:118: in _test
    assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
tests/layer_tests/common/layer_test_class.py:160: in compare_ie_results_with_framework
    if not allclose(infer_res[ie_out_name], framework_res[framework_out_name],
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cur_array = array([2, 5, 3, 7, 7, 6, 4, 1])
ref_array = array([5, 7, 0, 2, 2, 1, 5, 3, 0]), atol = 0.0001, rtol = 0.0001

    def allclose(cur_array, ref_array, atol, rtol):
        """
        Comparison of abs_diff and rel_diff with tolerances for every values of corresponding elements.
        If (abs_diff &lt; atol) or (rel_diff &lt; rtol) for every element, comparison of elements will pass, else will fail.
        Note: if value is very small, firstly abs_diff will be used. If value is huge, abs_diff may be failed,
        and rel_diff will be used. So if tensor has small and huge values, need to compare every value
        with abs_diff and rel_diff instead of using one of it for the whole array.
        :param cur_array: tensor from IE
        :param ref_array: tensor from FW
        :param atol: absolute tolerance (threshold for absolute difference)
        :param rtol: relative tolerance (threshold for relative difference)
        :return: bool value means that values of tensors are equal with tolerance or not
        """
        if cur_array.dtype == bool:
            abs_diff = np.absolute(cur_array ^ ref_array)
        else:
&gt;           abs_diff = np.absolute(cur_array - ref_array)
E           ValueError: operands could not be broadcast together with shapes (8,) (9,)

tests/layer_tests/common/utils/common_utils.py:102: ValueError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.490"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2],&#10;        [2],&#10;        [2],&#10;        [2]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [2],&#10;        [0],&#10;        [2]]])}; framework_res={'ArgValue': array([[[0],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[0],&#10;        [2],&#10;        [0],&#10;        [0]],&#10;&#10;       [[1],&#10;        [1],&#10;        [0],&#10;        [2]],&#10;&#10;       [[1],&#10;        [1],&#10;        [1],&#10;        [1]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbbd60&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuedgsutxe_'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbbd60&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuedgsutxe_'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuedgsutxe_', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2],
E               [2],
E               [2],
E               [2]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [2],
E               [0],
E               [2]]])}; framework_res={'ArgValue': array([[[0],
E               [0],
E               [1],
E               [1]],
E       
E              [[0],
E               [2],
E               [0],
E               [0]],
E       
E              [[1],
E               [1],
E               [0],
E               [2]],
E       
E              [[1],
E               [1],
E               [1],
E               [1]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.431"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a842b0&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueytq3iy80'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c563550&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...-6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.552"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([5, 7, 0, 2, 2, 1, 5, 3, 0])}; framework_res={'ArgValue': array([7, 4, 2, 4, 3, 0, 4, 2, 2])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3fac0&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueyjp9odcy'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3fac0&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueyjp9odcy'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueyjp9odcy', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([5, 7, 0, 2, 2, 1, 5, 3, 0])}; framework_res={'ArgValue': array([7, 4, 2, 4, 3, 0, 4, 2, 2])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.418"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf84f0&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuev80hfajf'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff530312f80&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.566"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[0],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[0],&#10;        [2],&#10;        [0],&#10;        [0]],&#10;&#10;       [[1],&#10;        [1],&#10;        [0],&#10;        [2]],&#10;&#10;       [[1],&#10;        [1],&#10;        [1],&#10;        [1]]])}; framework_res={'ArgValue': array([[[1, 2, 0]],&#10;&#10;       [[2, 2, 2]],&#10;&#10;       [[1, 0, 1]],&#10;&#10;       [[1, 3, 2]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84070&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuef8lpp03t'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84070&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuef8lpp03t'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuef8lpp03t', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[0],
E               [0],
E               [1],
E               [1]],
E       
E              [[0],
E               [2],
E               [0],
E               [0]],
E       
E              [[1],
E               [1],
E               [0],
E               [2]],
E       
E              [[1],
E               [1],
E               [1],
E               [1]]])}; framework_res={'ArgValue': array([[[1, 2, 0]],
E       
E              [[2, 2, 2]],
E       
E              [[1, 0, 1]],
E       
E              [[1, 3, 2]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.546"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([7, 3, 2, 4, 6, 2, 7, 0])}; framework_res={'ArgValue': array([2, 5, 3, 7, 7, 6, 4, 1])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3f220&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueun2pg90i'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3f220&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueun2pg90i'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueun2pg90i', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([7, 3, 2, 4, 6, 2, 7, 0])}; framework_res={'ArgValue': array([2, 5, 3, 7, 7, 6, 4, 1])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.456"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf88e0&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value7k1jniz9'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff528749390&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.456"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a847c0&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuertqgqivo'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c76d4b0&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.434"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7cac0&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value_7idm439'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e43d15d0&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5....3., -7.]],

        [[ 1.,  7., -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.421"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf82b0&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuew4zaja7v'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff5302d9e70&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...-6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.456"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84a90&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuevt9p71pc'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c5cffa0&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5....3., -7.]],

        [[ 1.,  7., -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.451"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c250&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuethhvjb4g'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e4743910&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.537" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.481"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84a00&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value_nvyoayb'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c76f430&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5....3., -7.]],

        [[ 1.,  7., -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.428"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c7f0&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueg9_smr7r'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e4749a50&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.587" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.439"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84610&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuezrtazmqb'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c5cf8e0&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.559"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([7, 4, 2, 4, 3, 0, 4, 2, 2])}; framework_res={'ArgValue': array([5, 7, 0, 2, 2, 1, 5, 3, 0])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3f9a0&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value9uybybro'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb3f9a0&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP32', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value9uybybro'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = False
mo_params = {'compress_to_fp16': False, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mod...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_value9uybybro', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([7, 4, 2, 4, 3, 0, 4, 2, 2])}; framework_res={'ArgValue': array([5, 7, 0, 2, 2, 1, 5, 3, 0])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.457"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8610&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuehruv_qi3'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff531cfb490&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.570" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.194"><failure message="struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cbba00&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valuen9jqwn_n'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:79: in _test
    self.check_tflite_model_has_only_allowed_ops()
tests/layer_tests/common/tflite_layer_test_class.py:54: in check_tflite_model_has_only_allowed_ops
    model = utils.read_model(self.model_path)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:61: in read_model
    model = convert_bytearray_to_object(model_bytearray)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/tools/flatbuffer_utils.py:40: in convert_bytearray_to_object
    model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17784: in GetRootAsModel
    return cls.GetRootAs(buf, offset)
../../../.local/lib/python3.10/site-packages/tensorflow/lite/python/schema_py_generated.py:17776: in GetRootAs
    n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

packer_type = &lt;_struct.Struct object at 0x7ff534c210d0&gt;, buf = bytearray(b'')
head = 0

    def Get(packer_type, buf, head):
        """ Get decodes a value at buf[head] using `packer_type`. """
&gt;       return packer_type.unpack_from(memoryview_type(buf), head)[0]
E       struct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)

../../../.local/lib/python3.10/site-packages/flatbuffers/encode.py:26: error</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.450"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c2e0&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value9rbqfp3t'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41edb7db40&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...-6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.550" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.430"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c5b0&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueo96h37if'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41ec153250&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.606" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.455"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c400&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value4gxkon2k'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e43d3970&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...-6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.444"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84340&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuemmnhxa9k'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c515060&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...-6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.443"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c910&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueoz057tzu'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e4740f70&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5....3., -7.]],

        [[ 1.,  7., -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.435"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84970&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7fbd60497130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuea4fputeg'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c517c10&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5....3., -7.]],

        [[ 1.,  7., -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.431"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c760&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueqecj2d9u'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e4748b20&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.549"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7fbd44a84850&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueuv189j2q'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7fbd3c76d120&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.782"><failure message="AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2],&#10;        [2],&#10;        [2],&#10;        [2]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [0],&#10;        [1],&#10;        [1]],&#10;&#10;       [[2],&#10;        [2],&#10;        [0],&#10;        [2]]])}; framework_res={'ArgValue': array([[[2, 3, 2]],&#10;&#10;       [[1, 1, 3]],&#10;&#10;       [[3, 1, 0]],&#10;&#10;       [[2, 2, 0]]])}.">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c1c0&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueckwmyktu'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c1c0&gt;
framework_model = node {
  name: "Input"
  op: "Placeholder"
  attr {
    key: "shape"
    value {
      shape {
        dim {
         ...DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
}
versions {
  producer: 1645
}

ref_net = None, ie_device = 'CPU', precision = 'FP16', ir_version = None
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueckwmyktu'
use_old_api = False, use_new_frontend = True, infer_timeout = 60
enabled_transforms = '', disabled_transforms = ''
kwargs = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
model_path = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/model.tflite'
compress_to_fp16 = True
mo_params = {'compress_to_fp16': True, 'input_model': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/mode...'output_dir': '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_arg_valueckwmyktu', ...}
exit_code = 0, stderr = ''

    def _test(self, framework_model, ref_net, ie_device, precision, ir_version, temp_dir, use_old_api,
              use_new_frontend=True, infer_timeout=60, enabled_transforms='',
              disabled_transforms='', **kwargs):
        """
        :param enabled_transforms/disabled_transforms: string with idxs of transforms that should be enabled/disabled.
                                                       Example: "transform_1,transform_2"
        """
        model_path = self.produce_model_path(framework_model=framework_model, save_path=temp_dir)
        self.use_new_frontend = use_new_frontend
        self.use_old_api = use_old_api
        # TODO Pass environment variables via subprocess environment
        os.environ['MO_ENABLED_TRANSFORMS'] = enabled_transforms
        os.environ['MO_DISABLED_TRANSFORMS'] = disabled_transforms
    
        compress_to_fp16 = False if precision == 'FP32' else True
        mo_params = {self.input_model_key: model_path,
                     "output_dir": temp_dir,
                     "compress_to_fp16": compress_to_fp16,
                     "model_name": 'model'}
    
        if 'input_shapes' in kwargs and len(kwargs['input_shapes']):
            input_shapes_str = []
            for ishape in kwargs['input_shapes']:
                input_shapes_str.append('[' + ','.join([str(i) for i in ishape]) + ']')
            mo_params.update(dict(input_shape=','.join(input_shapes_str)))
    
        if 'input_names' in kwargs and len(kwargs['input_names']):
            mo_params.update(dict(input=','.join(kwargs['input_names'])))
    
        if use_new_frontend:
            mo_params["use_new_frontend"] = True
        else:
            mo_params["use_legacy_frontend"] = True
    
        exit_code, stderr = generate_ir_python_api(**mo_params)
    
        del os.environ['MO_ENABLED_TRANSFORMS']
        del os.environ['MO_DISABLED_TRANSFORMS']
        assert not exit_code, (
            "IR generation failed with {} exit code: {}".format(exit_code, stderr))
    
        path_to_xml = Path(temp_dir, 'model.xml')
        path_to_bin = Path(temp_dir, 'model.bin')
    
        # TODO: need to update ref graphs or get rid of this comparison
        # if ref_net is not None:
        #     ir = IREngine(path_to_xml, path_to_bin, precision=precision)
        #     (flag, resp) = ir.compare(ref_net)
        #     assert flag, '\n'.join(resp)
    
        config = None
        # GPU default execution precision is FP16, so if we want to check FP32 inference
        # we need to set explicit precision hint
        if ie_device == 'GPU' and precision == 'FP32':
            config = {'INFERENCE_PRECISION_HINT': 'f32'}
    
        if self.use_old_api:
            ie_engine = IEInfer(model=path_to_xml,
                                weights=path_to_bin,
                                device=ie_device)
        else:
            ie_engine = InferAPI20(model=path_to_xml,
                                   weights=path_to_bin,
                                   device=ie_device,
                                   use_new_frontend=use_new_frontend)
        # Prepare feed dict
        if 'kwargs_to_prepare_input' in kwargs and kwargs['kwargs_to_prepare_input']:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision),
                                              kwargs['kwargs_to_prepare_input'])
        else:
            inputs_dict = self._prepare_input(ie_engine.get_inputs_info(precision))
    
        # IE infer:
        infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
    
        if hasattr(self, 'skip_framework') and self.skip_framework:
            warnings.warn('Framework is skipped')
            return
    
        # Framework infer:
        fw_res = self.get_framework_results(inputs_dict=inputs_dict, model_path=model_path)
    
        if 'custom_eps' in kwargs and kwargs['custom_eps'] is not None:
            custom_eps = kwargs['custom_eps']
        else:
            if precision == 'FP32':
                custom_eps = 1e-4
            else:
                custom_eps = 5e-2
        # Compare Ie results with Framework results
&gt;       assert self.compare_ie_results_with_framework(infer_res=infer_res, framework_res=fw_res,
                                                      framework_eps=custom_eps), \
            "Comparing with Framework failed: ie_res={}; framework_res={}.".format(infer_res,
                                                                                   fw_res)
E       AssertionError: Comparing with Framework failed: ie_res={'ArgValue': array([[[2],
E               [2],
E               [2],
E               [2]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [0],
E               [1],
E               [1]],
E       
E              [[2],
E               [2],
E               [0],
E               [2]]])}; framework_res={'ArgValue': array([[[2, 3, 2]],
E       
E              [[1, 1, 3]],
E       
E              [[3, 1, 0]],
E       
E              [[2, 2, 0]]])}.

tests/layer_tests/common/layer_test_class.py:118: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7fbd60497640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.731" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.542" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'shape': [2, 4, 5, 1]} ]" time="0.017"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0660&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0660&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac300&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7fbd466ac300&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a46ad0&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_nbltd7ud0'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a46ad0&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0660&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0660&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac300&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7fbd466ac300&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'shape': [2, 1]} ]" time="0.012"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0740&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0740&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac780&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7fbd466ac780&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a46dd0&gt;
params = {'shape': [2, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_na6ec0whn'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a46dd0&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0740&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0740&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac780&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7fbd466ac780&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'shape': [2, 1]} ]" time="0.012"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0040&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0040&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac780&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7fbd466ac780&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a466b0&gt;
params = {'shape': [2, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nf1bm_s1v'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a466b0&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0040&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0040&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac780&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7fbd466ac780&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'shape': [2, 1]} ]" time="0.010"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0900&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0900&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac780&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7fbd466ac780&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a464d0&gt;
params = {'shape': [2, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_nj3rqso6p'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a464d0&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0900&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0900&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac780&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7fbd466ac780&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'shape': [2, 4, 5, 1]} ]" time="0.011"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f1a80&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7fbd3c3f1a80&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac300&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7fbd466ac300&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a467d0&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_ng29yuo8i'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a467d0&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f1a80&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7fbd3c3f1a80&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac300&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7fbd466ac300&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'shape': [2, 4, 5, 1]} ]" time="0.011"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0f20&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0f20&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac300&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7fbd466ac300&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a46ec0&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nfxek_ju5'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a46ec0&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f0f20&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7fbd3c3f0f20&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac300&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7fbd466ac300&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'shape': [2, 1]} ]" time="0.024"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c757140&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7fbd3c757140&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac780&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7fbd466ac780&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a468c0&gt;
params = {'shape': [2, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n_bybdfv0'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a468c0&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c757140&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7fbd3c757140&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac780&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7fbd466ac780&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'shape': [2, 4, 5, 1]} ]" time="0.024"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f17e0&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7fbd3c3f17e0&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac300&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7fbd466ac300&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a465c0&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_nrlver55g'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7fbd44a465c0&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7fbd3c3f17e0&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7fbd3c3f17e0&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7fbd466ac300&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7fbd466ac300&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.388"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c6d0&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value9__is99x'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41ec0a7760&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.456"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c9a0&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuetgcbtqwc'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41ec0a7790&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5....3., -7.]],

        [[ 1.,  7., -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.531" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.520" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.446"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c490&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'op_name': 'ARG_MAX', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_value0afith2v'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41ec180310&gt;&gt;
func_args = [{'Input': array([[[[ 5.,  4.,  1.]],

        [[ 1.,  0., -3.]],

        [[ 8.,  9., -4.]],

        [[-1.,  2., -5.... -9.]],

        [[-5.,  4., -2.]],

        [[ 2.,  4., -3.]]]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.511" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.520" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [4, 4, 1, 3], 'axis': None} ]" time="0.402"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8a90&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [4, 4, 1, 3]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuet3415qi2'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff5303cf670&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.503" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.419"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf87c0&gt;
params = {'axis': -1, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuevpqgd9e2'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff528717490&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'shape': [9, 8], 'axis': -1} ]" time="0.413"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7f41edb7c880&gt;
params = {'axis': -1, 'op_func': &lt;function argmin_v2 at 0x7f420959f640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueyj3c0f3e'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7f41e43835e0&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP16 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.383"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf8730&gt;
params = {'axis': None, 'op_func': &lt;function argmax_v2 at 0x7ff54d9db130&gt;, 'op_name': 'ARG_MAX', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valuencvfiwhc'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff5302ff5b0&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...     [ 8., -5.,  9., -8., -4., -6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, None]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:CPU - precision:FP32 - params:{'op_name': 'ARG_MAX', 'op_func': &lt;function argmax_v2 at 0x7f420959f130&gt;, 'shape': [4, 4, 1, 3], 'axis': -1} ]" time="0.450" /><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'shape': [2, 4, 5, 1]} ]" time="0.010"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458d380&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7f41e458d380&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3eb00&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_n82t2tsrh'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3eb00&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458d380&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7f41e458d380&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_ArgValue.TestTFLiteArgValueLayerTest" name="test_arg_value[ ie_device:GPU - precision:FP32 - params:{'op_name': 'ARG_MIN', 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'shape': [9, 8], 'axis': None} ]" time="0.355"><failure message="multiprocessing.context.ProcessError: &#10;Inference Engine running failed: &#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py&quot;, line 34, in _mp_wrapped_func&#10;    res = func(*func_args)&#10;  File &quot;/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py&quot;, line 88, in fw_infer&#10;    exec_net = ie.compile_model(net, self.device, config)&#10;  File &quot;/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py&quot;, line 543, in compile_model&#10;    super().compile_model(model, device_name, {} if config is None else config),&#10;&#10;RuntimeError: Exception from src/inference/src/core.cpp:113:&#10;[ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:&#10;Cannot get fallback device for index: 0. The total number of found devices is 0">self = &lt;test_tfl_ArgValue.TestTFLiteArgValueLayerTest object at 0x7ff531cf83d0&gt;
params = {'axis': None, 'op_func': &lt;function argmin_v2 at 0x7ff54d9db640&gt;, 'op_name': 'ARG_MIN', 'shape': [9, 8]}
ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_arg_valueelo4y3_8'

    @pytest.mark.parametrize("params", sorted(test_data, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_arg_value(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_ArgValue.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:80: in _test
    super()._test(model, None, ie_device, precision, None, temp_dir, False, True, **params)
tests/layer_tests/common/layer_test_class.py:101: in _test
    infer_res = ie_engine.infer(input_data=inputs_dict, infer_timeout=infer_timeout, config=config)
tests/layer_tests/common/layer_utils.py:33: in infer
    self.res = multiprocessing_run(self.fw_infer, [input_data, config], self.name, infer_timeout)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

func = &lt;bound method InferAPI20.fw_infer of &lt;common.layer_utils.InferAPI20 object at 0x7ff53035bb80&gt;&gt;
func_args = [{'Input': array([[ 5.,  4.,  1.,  1.,  0., -3.,  8.,  9.],
       [-4., -1.,  2., -5.,  6.,  4., -6., -9.],
       [-...-6.,  9., -8.],
       [ 0.,  4.,  9.,  5.,  0.,  6.,  2.,  1.]], dtype=float32)}, {'INFERENCE_PRECISION_HINT': 'f32'}]
func_log_name = 'Inference Engine', timeout = 60

    def multiprocessing_run(func: Callable, func_args: list, func_log_name: str, timeout: Union[int, None] = None):
        """
        Wraps callable object to a separate process using multiprocessing module
        :param func: callable object
        :param func_args: list of arguments for callable
        :param func_log_name: name of callable used for logging
        :param timeout: positive int to limit execution time
        :return: return value (or values) from callable object
        """
        queue = Queue()
        logger_queue = Queue(-1)
        process = Process(target=_mp_wrapped_func, args=(func, func_args, queue, logger_queue))
        process.start()
        try:
            error_message, *ret_args = queue.get(timeout=timeout)
        except QueueEmpty:
            raise TimeoutError("{func} running timed out!".format(func=func_log_name))
        finally:
            queue.close()
    
            # Extract logs from Queue and pass to root logger
            while not logger_queue.empty():
                rec = logger_queue.get()
                log.getLogger().handle(rec)
            logger_queue.close()
    
            if process.is_alive():
                process.terminate()
                process.join()
            else:
                exit_signal = multiprocessing_exitcode_to_signal(process.exitcode)
                if exit_signal:
                    raise ProcessError(
                        "{func} was killed with a signal {signal}".format(func=func_log_name, signal=exit_signal))
    
        if error_message:
&gt;           raise ProcessError("\n{func} running failed: \n{msg}".format(func=func_log_name, msg=error_message))
E           multiprocessing.context.ProcessError: 
E           Inference Engine running failed: 
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/utils/multiprocessing_utils.py", line 34, in _mp_wrapped_func
E               res = func(*func_args)
E             File "/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/layer_utils.py", line 88, in fw_infer
E               exec_net = ie.compile_model(net, self.device, config)
E             File "/home/chaitanyasai/.local/lib/python3.10/site-packages/openvino/runtime/ie_api.py", line 543, in compile_model
E               super().compile_model(model, device_name, {} if config is None else config),
E           
E           RuntimeError: Exception from src/inference/src/core.cpp:113:
E           [ GENERAL_ERROR ] Check 'all_devices.size() &gt; idx' failed at src/plugins/proxy/src/plugin.cpp:512:
E           Cannot get fallback device for index: 0. The total number of found devices is 0

tests/layer_tests/common/utils/multiprocessing_utils.py:78: ProcessError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'shape': [2, 1]} ]" time="0.009"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458c820&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7f41e458c820&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3ee00&gt;
params = {'shape': [2, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_n20b278hu'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3ee00&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458c820&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7f41e458c820&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'shape': [2, 1]} ]" time="0.010"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458c660&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7f41e458c660&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e6e0&gt;
params = {'shape': [2, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_no1lij69q'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e6e0&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458c660&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7f41e458c660&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'shape': [2, 1]} ]" time="0.015"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458d380&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7f41e458d380&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e500&gt;
params = {'shape': [2, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_n9vphag9g'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e500&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458d380&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7f41e458d380&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'shape': [2, 4, 5, 1]} ]" time="0.012"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458d7e0&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7f41e458d7e0&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e800&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n6g1wfxs7'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e800&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458d7e0&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7f41e458d7e0&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'shape': [2, 4, 5, 1]} ]" time="0.012"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458db60&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7f41e458db60&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3eef0&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n865zmvbf'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3eef0&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458db60&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7f41e458db60&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'shape': [2, 1]} ]" time="0.012"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41ed9e27a0&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7f41ed9e27a0&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e8f0&gt;
params = {'shape': [2, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_n7j501zp4'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e8f0&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41ed9e27a0&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7f41ed9e27a0&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7f41ef7c0880&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'shape': [2, 4, 5, 1]} ]" time="0.025"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458d1c0&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7f41e458d1c0&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e5f0&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_n_b_nkz8o'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7f41edb3e5f0&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7f41e458d1c0&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7f41e458d1c0&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7f41ef7c0400&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'shape': [2, 4, 5, 1]} ]" time="0.012"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff5304865e0&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7ff5304865e0&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20780&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7ff533b20780&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cbab60&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_nql0z4pat'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cbab60&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff5304865e0&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7ff5304865e0&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20780&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7ff533b20780&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP32 - params:{'shape': [2, 1]} ]" time="0.011"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff531d5f3e0&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7ff531d5f3e0&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20c00&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7ff533b20c00&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cbae60&gt;
params = {'shape': [2, 1]}, ie_device = 'CPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_np2kd6d86'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cbae60&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff531d5f3e0&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7ff531d5f3e0&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20c00&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7ff533b20c00&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'shape': [2, 1]} ]" time="0.009"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff5304858c0&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7ff5304858c0&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20c00&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7ff533b20c00&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba740&gt;
params = {'shape': [2, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nyxxgtcvh'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba740&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff5304858c0&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7ff5304858c0&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20c00&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7ff533b20c00&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'shape': [2, 1]} ]" time="0.011"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff530487d80&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7ff530487d80&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20c00&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7ff533b20c00&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba560&gt;
params = {'shape': [2, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_n_57e4gpk'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba560&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff530487d80&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7ff530487d80&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20c00&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7ff533b20c00&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP32 - params:{'shape': [2, 4, 5, 1]} ]" time="0.012"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff5304874c0&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7ff5304874c0&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20780&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7ff533b20780&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba860&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP32'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_ni_ql_0hz'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba860&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff5304874c0&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7ff5304874c0&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20780&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7ff533b20780&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'shape': [2, 4, 5, 1]} ]" time="0.020"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff530487680&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7ff530487680&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20780&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7ff533b20780&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cbaf50&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nq18fg99x'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cbaf50&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff530487680&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7ff530487680&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20780&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7ff533b20780&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:GPU - precision:FP16 - params:{'shape': [2, 1]} ]" time="0.053"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff531d06880&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7ff531d06880&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20c00&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7ff533b20c00&gt; = {'shape': [2, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba950&gt;
params = {'shape': [2, 1]}, ie_device = 'GPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/GPU_test_add_nw97yyz3r'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba950&gt;
params = {'shape': [2, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff531d06880&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7ff531d06880&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20c00&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7ff533b20c00&gt; = {'shape': [2, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase><testcase classname="tensorflow_lite_tests.dummy.test_tfl_AddN.TestTFLiteAddNLayerTest" name="test_add_n[ ie_device:CPU - precision:FP16 - params:{'shape': [2, 4, 5, 1]} ]" time="0.038"><failure message="AssertionError: Unexpected parameters for test: shape&#10;assert 1 == 2&#10; +  where 1 = len({'shape'})&#10; +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff530485d20&gt;({'num_inputs', 'shape'})&#10; +      where &lt;built-in method intersection of set object at 0x7ff530485d20&gt; = {'shape'}.intersection&#10; +        where {'shape'} = set(dict_keys(['shape']))&#10; +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20780&gt;()&#10; +            where &lt;built-in method keys of dict object at 0x7ff533b20780&gt; = {'shape': [2, 4, 5, 1]}.keys">self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba650&gt;
params = {'shape': [2, 4, 5, 1]}, ie_device = 'CPU', precision = 'FP16'
temp_dir = '/home/chaitanyasai/Desktop/G_Soc/openvino/tests/layer_tests/common/out/CPU_test_add_n9znus27p'

    @pytest.mark.parametrize("params", sorted(test_params, key=lambda x: len(x['shape'])))
    @pytest.mark.nightly
    def test_add_n(self, params, ie_device, precision, temp_dir):
&gt;       self._test(ie_device, precision, temp_dir, params)

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/layer_tests/common/tflite_layer_test_class.py:77: in _test
    model = self.make_model(params)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;test_tfl_AddN.TestTFLiteAddNLayerTest object at 0x7ff531cba650&gt;
params = {'shape': [2, 4, 5, 1]}

    def make_model(self, params):
&gt;       assert len(set(params.keys()).intersection({'shape', 'num_inputs'})) == 2, \
            'Unexpected parameters for test: ' + ','.join(params.keys())
E       AssertionError: Unexpected parameters for test: shape
E       assert 1 == 2
E        +  where 1 = len({'shape'})
E        +    where {'shape'} = &lt;built-in method intersection of set object at 0x7ff530485d20&gt;({'num_inputs', 'shape'})
E        +      where &lt;built-in method intersection of set object at 0x7ff530485d20&gt; = {'shape'}.intersection
E        +        where {'shape'} = set(dict_keys(['shape']))
E        +          where dict_keys(['shape']) = &lt;built-in method keys of dict object at 0x7ff533b20780&gt;()
E        +            where &lt;built-in method keys of dict object at 0x7ff533b20780&gt; = {'shape': [2, 4, 5, 1]}.keys

tests/layer_tests/tensorflow_lite_tests/dummy/test_tfl_AddN.py:28: AssertionError</failure></testcase></testsuite></testsuites>